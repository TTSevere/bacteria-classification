{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "'C4-7' vs 'C5' vs all_other.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL0TE_c0qu_N",
        "colab_type": "text"
      },
      "source": [
        "###  <span style=\"color:red\">**This Notebook can be run from Google Colab:**</span>\n",
        "\n",
        "https://colab.research.google.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn7wUdlaqu_V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "16ec0a44-2525-4763-b965-48bb83f05f1f"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "import keras\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.layers import Input, Dense, Activation, Dropout, BatchNormalization,\\\n",
        "                          Conv2D, MaxPooling2D, Flatten, AveragePooling2D,\\\n",
        "                          GlobalAveragePooling2D, ZeroPadding2D\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import RMSprop, Adam, Adamax, Nadam, SGD\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, \\\n",
        "                            classification_report\n",
        "\n",
        "# Import PyDrive and associated libraries (to connect with GoogleDrive)\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# disable warnings\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DItexGaqdfEA"
      },
      "source": [
        "### **Check if we are using GPU:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vn7AYx74dNq6",
        "outputId": "75110118-5120-46ca-c1df-ede71d87d5bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import backend as K\n",
        "if K.backend() == \"tensorflow\":\n",
        "    import tensorflow as tf\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "    if device_name == '':\n",
        "        device_name = \"None\"\n",
        "    print('Using TensorFlow version:', tf.__version__, ', GPU:', device_name)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow version: 1.15.0 , GPU: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QQXfhMrHlqrz"
      },
      "source": [
        "### **Download Patches from GoogleDrive:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ACvUlEucnnY0",
        "outputId": "54543d0f-4a57-4236-e802-1115bc0e96b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = '1GrvQWYf90c9lE-qPCovkZA8Ai0cayzkP'\n",
        "\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile(downloaded['title'])\n",
        "print('Downloaded content: \"{}\"'.format(downloaded['title']))\n",
        "print('Root dir content: {}'.format(os.listdir()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded content: \"Img_Patches_256.zip\"\n",
            "Root dir content: ['.config', 'Img_Patches_256.zip', 'adc.json', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HSdGQav-qEJM"
      },
      "source": [
        "### **Unzip the Patches:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jP1-2THkn47w",
        "outputId": "8e33a62f-31f6-401e-8b62-bb78469755af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Remove 'Patches' dir if it already exists\n",
        "if 'Patches' in os.listdir():\n",
        "  shutil.rmtree('./Patches')\n",
        "with zipfile.ZipFile(downloaded['title'],\"r\") as zip:\n",
        "    zip.extractall()\n",
        "os.remove(downloaded['title'])\n",
        "print('Root dir content: {}'.format(os.listdir()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root dir content: ['.config', 'content', 'adc.json', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tLdDGys5Upq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2586599-0f98-4fca-9bc6-4222d9fa7c86"
      },
      "source": [
        "cd content/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nd7htfD3rJ6V"
      },
      "source": [
        "### **Let's count patches by type and class:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DjJy7FFsvm5H",
        "outputId": "3bff67fc-91fa-4bf1-a55b-5f9edb899fc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "class_weights = {} # empty dictionary to store class weights\n",
        "classes = ['1','2','3','4','5','6','7','0']\n",
        "\n",
        "grand_total, pos_total, neg_total = 0, 0, 0\n",
        "for type in ['Serial', 'Control', 'Streak']:\n",
        "    print(\"\\nTotal '{}' Patches per location:\".format(type))\n",
        "    n_type, type_pos, type_neg = 0, 0, 0\n",
        "    class_weights[type] = {} # nested empty dictionary to store class weights\n",
        "    # class_weights[type]['pos'] = {} # nested dictionary to store class weights\n",
        "    # class_weights[type]['neg'] = {} # nested dictionary to store class weights\n",
        "    for cls in classes:\n",
        "        pfolder = './Img_Patches/{}/{}'.format(type,cls)\n",
        "        n = len(os.listdir(pfolder))\n",
        "        # total = n_pos + n_neg\n",
        "        n_type += n\n",
        "        # type_pos += n_pos\n",
        "        # type_neg += n_neg\n",
        "        print('total_{}: {} = {} positive + {} negative'.format(cls,n,n,0))\n",
        "        class_weights[type]['{}'.format(cls)] = 1/n if n else 0\n",
        "    print('Total {}: {} = {} positive + {} negative'.format(type,n_type,n_type,0))\n",
        "    for loc in class_weights[type].keys():\n",
        "        class_weights[type][loc] *= n_type\n",
        "    grand_total += n_type\n",
        "print('\\nGRAND TOTAL: {} = {} positive + {} negative'.format(grand_total,grand_total,0))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total 'Serial' Patches per location:\n",
            "total_1: 106 = 106 positive + 0 negative\n",
            "total_2: 162 = 162 positive + 0 negative\n",
            "total_3: 614 = 614 positive + 0 negative\n",
            "total_4: 303 = 303 positive + 0 negative\n",
            "total_5: 108 = 108 positive + 0 negative\n",
            "total_6: 169 = 169 positive + 0 negative\n",
            "total_7: 3 = 3 positive + 0 negative\n",
            "total_0: 418 = 418 positive + 0 negative\n",
            "Total Serial: 1883 = 1883 positive + 0 negative\n",
            "\n",
            "Total 'Control' Patches per location:\n",
            "total_1: 13 = 13 positive + 0 negative\n",
            "total_2: 20 = 20 positive + 0 negative\n",
            "total_3: 77 = 77 positive + 0 negative\n",
            "total_4: 40 = 40 positive + 0 negative\n",
            "total_5: 10 = 10 positive + 0 negative\n",
            "total_6: 19 = 19 positive + 0 negative\n",
            "total_7: 3 = 3 positive + 0 negative\n",
            "total_0: 106 = 106 positive + 0 negative\n",
            "Total Control: 288 = 288 positive + 0 negative\n",
            "\n",
            "Total 'Streak' Patches per location:\n",
            "total_1: 305 = 305 positive + 0 negative\n",
            "total_2: 431 = 431 positive + 0 negative\n",
            "total_3: 880 = 880 positive + 0 negative\n",
            "total_4: 522 = 522 positive + 0 negative\n",
            "total_5: 279 = 279 positive + 0 negative\n",
            "total_6: 378 = 378 positive + 0 negative\n",
            "total_7: 56 = 56 positive + 0 negative\n",
            "total_0: 478 = 478 positive + 0 negative\n",
            "Total Streak: 3329 = 3329 positive + 0 negative\n",
            "\n",
            "GRAND TOTAL: 5500 = 5500 positive + 0 negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TLfSmpKFPLFu"
      },
      "source": [
        "### **Let's downsample training ('Serial') classes to no more than the minimum between 'C4-7' and 'C5':**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IPt4zNrWPJed",
        "outputId": "f18f2445-d497-4df9-f6bd-ef6190f5be60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "train_type = 'Serial'\n",
        "print('Before downsampling training patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pos_folder = './Img_Patches/{}/{}'.format(train_type,cls)\n",
        "    n_pos = len(os.listdir(pos_folder))\n",
        "    total = n_pos\n",
        "    n_type += total\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,total,0))\n",
        "    totals['{}'.format(cls)] = n_pos if n_pos else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(train_type,n_type,n_type,0))\n",
        "\n",
        "if totals['4'] < totals['5']:\n",
        "    minority = '4'\n",
        "else:\n",
        "    minority = '5'\n",
        "n_min = totals[minority]\n",
        "for key, value in totals.items():\n",
        "    #if key != minority and key not in ['C4-7','C5']:\n",
        "    if key != minority:\n",
        "        n_to_delete = max(value-n_min, 0)\n",
        "        root = './Img_Patches/{}/{}/'.format(train_type,key)\n",
        "        patches = os.listdir(root)\n",
        "        patches_to_delete = np.random.choice(patches, n_to_delete, replace=False)\n",
        "        for patch in patches_to_delete:\n",
        "            pass\n",
        "            os.remove(root + patch)\n",
        "\n",
        "print('After downsampling training patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pos_folder = './Img_Patches/{}/{}'.format(train_type,cls)\n",
        "    n_pos = len(os.listdir(pos_folder))\n",
        "    total = n_pos\n",
        "    n_type += total\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,total,0))\n",
        "    totals['{}'.format(cls)] = n_pos if n_pos else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(train_type,n_type,n_type,0))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before downsampling training patches:\n",
            "total_1: 106 = 106 positive + 0 negative\n",
            "total_2: 162 = 162 positive + 0 negative\n",
            "total_3: 614 = 614 positive + 0 negative\n",
            "total_4: 303 = 303 positive + 0 negative\n",
            "total_5: 108 = 108 positive + 0 negative\n",
            "total_6: 169 = 169 positive + 0 negative\n",
            "total_7: 3 = 3 positive + 0 negative\n",
            "total_0: 418 = 418 positive + 0 negative\n",
            "Total Serial: 1883 = 1883 positive + 0 negative\n",
            "\n",
            "After downsampling training patches:\n",
            "total_1: 106 = 106 positive + 0 negative\n",
            "total_2: 108 = 108 positive + 0 negative\n",
            "total_3: 108 = 108 positive + 0 negative\n",
            "total_4: 108 = 108 positive + 0 negative\n",
            "total_5: 108 = 108 positive + 0 negative\n",
            "total_6: 108 = 108 positive + 0 negative\n",
            "total_7: 3 = 3 positive + 0 negative\n",
            "total_0: 108 = 108 positive + 0 negative\n",
            "Total Serial: 757 = 757 positive + 0 negative\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e8a8ZcfKKvw5"
      },
      "source": [
        "### **Let's move all patches for classes other than 'C4-7' or 'C5', to a single folder called 'all_other':**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dz9V2uuxLM50",
        "colab": {}
      },
      "source": [
        "for type_ in ['Serial', 'Control', 'Streak']:\n",
        "    folder = './Img_Patches/{}/'.format(type_)\n",
        "    if 'all_other' not in os.listdir(folder):\n",
        "        dest = folder + 'all_other'\n",
        "        os.mkdir(dest)\n",
        "\n",
        "    for cls in classes:\n",
        "        if cls in ['4','5']: continue\n",
        "        cls_folder = folder + cls\n",
        "        for patch in os.listdir(cls_folder):\n",
        "            source = cls_folder + '/' + patch\n",
        "            shutil.copy(source, dest)\n",
        "        shutil.rmtree(cls_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1sPAFxK7REM7"
      },
      "source": [
        "### **Let's count patches by type and class again:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmKlpYPZRE-_",
        "outputId": "1a4801d2-5b7a-4ddb-bca2-78ad6c841647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "class_weights = {} # empty dictionary to store class weights\n",
        "classes = ['4','5','all_other']\n",
        "grand_total, pos_total, neg_total = 0, 0, 0\n",
        "\n",
        "for type in ['Serial', 'Control', 'Streak']:\n",
        "    print(\"\\nTotal '{}' Patches per location:\".format(type))\n",
        "    n_type, type_pos, type_neg = 0, 0, 0\n",
        "    class_weights[type] = {} # nested empty dictionary to store class weights\n",
        "    for cls in classes:\n",
        "        pos_folder = './Img_Patches/{}/{}'.format(type,cls)\n",
        "        n_pos = len(os.listdir(pos_folder))\n",
        "        total = n_pos\n",
        "        n_type += total\n",
        "        print('total_{}: {} = {} positive + {} negative'.format(cls,total,total,0))\n",
        "        class_weights[type]['{}'.format(cls)] = 1/n_pos if n_pos else 0\n",
        "    print('Total {}: {} = {} positive + {} negative'.format(type,n_type,n_type,0))\n",
        "    for loc in class_weights[type].keys():\n",
        "        class_weights[type][loc] *= type_pos\n",
        "    grand_total += n_type\n",
        "print('\\nGRAND TOTAL: {} = {} positive + {} negative'.format(grand_total,grand_total,0))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total 'Serial' Patches per location:\n",
            "total_4: 108 = 108 positive + 0 negative\n",
            "total_5: 108 = 108 positive + 0 negative\n",
            "total_all_other: 541 = 541 positive + 0 negative\n",
            "Total Serial: 757 = 757 positive + 0 negative\n",
            "\n",
            "Total 'Control' Patches per location:\n",
            "total_4: 40 = 40 positive + 0 negative\n",
            "total_5: 10 = 10 positive + 0 negative\n",
            "total_all_other: 238 = 238 positive + 0 negative\n",
            "Total Control: 288 = 288 positive + 0 negative\n",
            "\n",
            "Total 'Streak' Patches per location:\n",
            "total_4: 522 = 522 positive + 0 negative\n",
            "total_5: 279 = 279 positive + 0 negative\n",
            "total_all_other: 2493 = 2493 positive + 0 negative\n",
            "Total Streak: 3294 = 3294 positive + 0 negative\n",
            "\n",
            "GRAND TOTAL: 4339 = 4339 positive + 0 negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vb9BMr39R9LH"
      },
      "source": [
        "### **Since we want to focus on 'C4-7' and 'C5', let's downsample again:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "31ZsitULR-Ap",
        "outputId": "5e329755-6455-445d-ea9b-b5a11fb5298d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "train_type = 'Serial'\n",
        "print('Before downsampling training patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pos_folder = './Img_Patches/{}/{}'.format(train_type,cls)\n",
        "    n_pos = len(os.listdir(pos_folder))\n",
        "    total = n_pos\n",
        "    n_type += total\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,total,0))\n",
        "    totals['{}'.format(cls)] = n_pos if n_pos else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(train_type,n_type,type_pos,type_neg))\n",
        "\n",
        "minority = min(totals, key=totals.get)\n",
        "n_min = totals[minority]\n",
        "for key, value in totals.items():\n",
        "    if key != minority and key not in ['4','5']:\n",
        "        n_to_delete = value - n_min\n",
        "        root = './Img_Patches/{}/{}/'.format(train_type,key)\n",
        "        patches = os.listdir(root)\n",
        "        patches_to_delete = np.random.choice(patches, n_to_delete, replace=False)\n",
        "        for patch in patches_to_delete:\n",
        "            pass\n",
        "            os.remove(root + patch)\n",
        "\n",
        "print('After downsampling training patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pos_folder = './Img_Patches/{}/{}'.format(train_type,cls)\n",
        "    n_pos = len(os.listdir(pos_folder))\n",
        "    total = n_pos\n",
        "    n_type += total\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,total,0))\n",
        "    class_weights[train_type]['{}'.format(cls)] = 1/n_pos if n_pos else 0\n",
        "    totals['{}'.format(cls)] = n_pos if n_pos else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(train_type,n_type,n_type,0))\n",
        "for loc in class_weights[train_type].keys():\n",
        "    class_weights[train_type][loc] *= n_type"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before downsampling training patches:\n",
            "total_4: 108 = 108 positive + 0 negative\n",
            "total_5: 108 = 108 positive + 0 negative\n",
            "total_all_other: 541 = 541 positive + 0 negative\n",
            "Total Serial: 757 = 0 positive + 0 negative\n",
            "\n",
            "After downsampling training patches:\n",
            "total_4: 108 = 108 positive + 0 negative\n",
            "total_5: 108 = 108 positive + 0 negative\n",
            "total_all_other: 108 = 108 positive + 0 negative\n",
            "Total Serial: 324 = 324 positive + 0 negative\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i2gctgbsF-fF"
      },
      "source": [
        "### **Let's downsample positive majority classes in validation ('Control')patches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tJETu-bSF_R8",
        "outputId": "d06477ea-003b-423b-ab53-2fdf9e6ac4fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "val_type = 'Control'\n",
        "print('Before downsampling validation patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pos_folder = './Img_Patches/{}/{}'.format(val_type,cls)\n",
        "    n_pos = len(os.listdir(pos_folder))\n",
        "    total = n_pos\n",
        "    n_type += total\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,total,0))\n",
        "    totals['{}'.format(cls)] = n_pos if n_pos else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(val_type,n_type,n_type,0))\n",
        "\n",
        "minority = min(totals, key=totals.get)\n",
        "n_min = totals[minority]\n",
        "for key, value in totals.items():\n",
        "    if key != minority:\n",
        "        n_to_delete = value - n_min\n",
        "        root = './Img_Patches/{}/{}/'.format(val_type,key)\n",
        "        patches = os.listdir(root)\n",
        "        patches_to_delete = np.random.choice(patches, n_to_delete, replace=False)\n",
        "        for patch in patches_to_delete:\n",
        "            os.remove(root + patch)\n",
        "\n",
        "print('After downsampling validation patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pos_folder = './Img_Patches/{}/{}'.format(val_type,cls)\n",
        "    n_pos = len(os.listdir(pos_folder))\n",
        "    total = n_pos\n",
        "    n_type += total\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,total,0))\n",
        "    totals['{}'.format(cls)] = n_pos if n_pos else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(val_type,n_type,n_type,0))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before downsampling validation patches:\n",
            "total_4: 40 = 40 positive + 0 negative\n",
            "total_5: 10 = 10 positive + 0 negative\n",
            "total_all_other: 238 = 238 positive + 0 negative\n",
            "Total Control: 288 = 288 positive + 0 negative\n",
            "\n",
            "After downsampling validation patches:\n",
            "total_4: 10 = 10 positive + 0 negative\n",
            "total_5: 10 = 10 positive + 0 negative\n",
            "total_all_other: 10 = 10 positive + 0 negative\n",
            "Total Control: 30 = 30 positive + 0 negative\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GNJ7lSKQpdOT"
      },
      "source": [
        "#### **Let's build image generators, using keras.preprocessing.image.ImageDataGenerator, rescaling image pixel values from [0,  255] to [0, 1]:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UANfdA6IUFKt",
        "outputId": "cbcd6db5-a496-4673-d2a0-944b72f0319c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "c1_pos_folder = './Img_Patches/Serial/5'\n",
        "img = plt.imread(c1_pos_folder + '/' + os.listdir(c1_pos_folder)[:5][0])\n",
        "img_size = img.shape\n",
        "train_batch_size = 32\n",
        "val_batch_size = 64\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "print(\"For training:\")\n",
        "train_generator = datagen.flow_from_directory(\n",
        "        './Img_Patches/Serial',\n",
        "        target_size=(img_size[0],img_size[1]),\n",
        "        batch_size=train_batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True)\n",
        "\n",
        "print(\"\\nFor validation:\")\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "        './Img_Patches/Control',\n",
        "        target_size=(img_size[0],img_size[1]),\n",
        "        batch_size=val_batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For training:\n",
            "Found 324 images belonging to 3 classes.\n",
            "\n",
            "For validation:\n",
            "Found 30 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "44ZzUGXvwqUn"
      },
      "source": [
        "#### **Let's check what is the training generator's index for each class, so we can correclty set up the class weights:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tSHQNC2awn84",
        "outputId": "87aa76d9-1f93-4260-bd26-f9e07b48ff5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print('train_generator.class_indices:', str(json.dumps(train_generator.class_indices, indent=2, default=str)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_generator.class_indices: {\n",
            "  \"4\": 0,\n",
            "  \"5\": 1,\n",
            "  \"all_other\": 2\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o30MT4e2O7gk"
      },
      "source": [
        "#### **Let's set up the class weights in correct order:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U2gsR9n6w2vm",
        "outputId": "fa6de317-2468-4761-bf26-3c474af9c172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "serial_pos_weights = []\n",
        "for cls in classes:\n",
        "    serial_pos_weights.append(class_weights['Serial']['{}'.format(cls)])\n",
        "print('original class weights dictionary:')\n",
        "print(str(json.dumps(class_weights['Serial'], indent=2, default=str)))\n",
        "print('class weights for generator, re-arranging indexes:')\n",
        "print(str(json.dumps(serial_pos_weights, indent=2, default=str)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original class weights dictionary:\n",
            "{\n",
            "  \"4\": 3.0,\n",
            "  \"5\": 3.0,\n",
            "  \"all_other\": 3.0\n",
            "}\n",
            "class weights for generator, re-arranging indexes:\n",
            "[\n",
            "  3.0,\n",
            "  3.0,\n",
            "  3.0\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gdXBPBsVxA39"
      },
      "source": [
        "## **Let's build our Base Model. We will use Resnet:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AjNmNKdHw4tM"
      },
      "source": [
        "#### **First. let's create a function to build a residual block.**\n",
        "\n",
        "#### We will use the residual block proposed in  [ResNetV2](https://arxiv.org/pdf/1603.05027.pdf) and will implement it by ourselves:\n",
        "\n",
        "\n",
        ">![Google's logo](https://camo.githubusercontent.com/7ae470c333cd76078e1c669055ad98bcedaf523f/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3130303532332f61313536613563322d303236622d646535352d613666622d6534666131373732623432632e706e67)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e-DqZH20w3ji",
        "colab": {}
      },
      "source": [
        "def res_block(X, filters, kernel_size=(3,3), l2_reg=1e-6, residual=True,\n",
        "              first=False, subsampling=False):\n",
        "    \"\"\"\n",
        "    Function to build a residual block as proposed in ResNetV2:\n",
        "             https://arxiv.org/pdf/1603.05027.pdf:\n",
        "    :param X: The input to the residual block\n",
        "    :param filters: Integer. Number of filters / channels in the output\n",
        "    :param kernel_size: Tuple (Int, Int). kernel size for convolution operations\n",
        "    :param l2_reg: Float. L2 norm for L2 regularization\n",
        "    :param residual: Boolean. True if residual block. Otherwise 'plain' block.\n",
        "    :param first: Boolean. True if first residual block -> ZeroPad and Maxpool.\n",
        "    :param subsampling: Boolean. True if subsampling within the residual block.\n",
        "    :return: the addition output from the residual block proposed in ResNetV2:\n",
        "              https://arxiv.org/pdf/1603.05027.pdf\n",
        "    \"\"\"\n",
        "    bn = BatchNormalization()(X)\n",
        "    relu = Activation(\"relu\")(bn)\n",
        "    \n",
        "    if first: #The first layer is subsampled with Maxpool\n",
        "      pad = ZeroPadding2D(padding=(1, 1))(relu)\n",
        "      relu = MaxPooling2D(pool_size=(3, 3), strides=(2,2))(pad)    \n",
        "    \n",
        "    if subsampling: #Resnet reduces size just by using stride=2 instead of pool\n",
        "      #Here we will reduce the size (subsample) by using stride 2 \n",
        "      conv_1 = Conv2D(filters, kernel_size, strides=(2,2), padding='same',\n",
        "                         kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                         kernel_initializer = glorot_uniform(0),\n",
        "                         bias_initializer = glorot_uniform(0))(relu)\n",
        "      if residual:\n",
        "        #To be able to add, we also need to reduce size of input\n",
        "        #For this, we will just use a 1x1 Conv2D with stride 2  \n",
        "        res = Conv2D(filters, kernel_size=[1,1], strides=(2,2),\n",
        "                     padding='same')(X)\n",
        "    else: #No subsampling, same size as input\n",
        "      conv_1 = Conv2D(filters, kernel_size, strides=(1,1), padding='same',\n",
        "                   kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                   kernel_initializer = glorot_uniform(0),\n",
        "                   bias_initializer = glorot_uniform(0))(relu)\n",
        "      if residual:\n",
        "        if first: #The first layer is subsampled with Maxpool so, resize X\n",
        "          #For this, we will just use a 1x1 Conv2D with stride 2\n",
        "          res = Conv2D(filters, kernel_size=[1,1], strides=(2,2),\n",
        "                     padding='same')(X)\n",
        "        else:\n",
        "          res = X\n",
        "    bn = BatchNormalization()(conv_1)\n",
        "    relu = Activation(\"relu\")(bn)\n",
        "\n",
        "    conv_2 = Conv2D(filters, kernel_size, padding='same',\n",
        "                       kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                       kernel_initializer = glorot_uniform(0),\n",
        "                       bias_initializer = glorot_uniform(0))(relu)\n",
        "    if residual:\n",
        "      add = keras.layers.add([res, conv_2])\n",
        "      return add\n",
        "    else:\n",
        "      return conv_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0K3-02NNUXOV"
      },
      "source": [
        "#### **Second, let's create a function to build a Resnet network:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "32s7rai-609p",
        "colab": {}
      },
      "source": [
        "#from keras.layers import AveragePooling2D, GlobalAveragePooling2D\n",
        "\n",
        "def make_resnet(img_size, n_classes, layers_per_group, n_filters, \n",
        "                    kernel_sizes, l2_reg=1e-6, optimizer=keras.optimizers.SGD,\n",
        "                    lr=1e-1, decay=1e-4, momentum=0.9, residual=True):\n",
        "  \n",
        "    \"\"\"\n",
        "    Function to build ResNet network, but with some user defined parameters.\n",
        "        \n",
        "        ResNet network input size is (224,224,3) then it starts with one \n",
        "        convolution layer, (7x7x64, stride 2) followed by maxpool (3x3, stride2) \n",
        "        then it will build 4 layer groups and the user will define the number of\n",
        "        residual layers per group.\n",
        "        \n",
        "        As example, ResNet34, after the first convolution layer, it has 4 groups\n",
        "        of layers with the following number of residual 'layers_per_group':\n",
        "        [6,8,12,6]\n",
        "        Because the residual connections are made between pair of layers, the\n",
        "        number of layers for each group must be a pair number.\n",
        "        \n",
        "        The user will also be able to define the number of filters and size of\n",
        "        each filter, independently for each group of layers. All layers in the\n",
        "        same group group will have the same number of filters and each filter\n",
        "        within the group will have the same size.\n",
        "        \n",
        "        Same as ResNet, an average pooling layer follows after the 4 groups\n",
        "        of layers.\n",
        "    \n",
        "    :param img_size: Size of input image, in the form: (size, size, #channels)\n",
        "    :param n_classes: Integer. Number of classes for classification.\n",
        "    :param layers_per_group: List with # of layers per group (e.g.[6,8,12,6])\n",
        "    :param n_filters: List of length 4, with number of filters for each group of\n",
        "                      layers.\n",
        "    :param kernel_sizes: List of length 4, with kernel sizes tuples for each \n",
        "                          group of layers\n",
        "    :param l2_reg: Float. L2 norm for L2 regularization\n",
        "    :param optimizer: A keras optimizer from keras.optimizers\n",
        "    :param lr: Float. learning rate for the optimizer.\n",
        "    :param decay: Float. learning rate decay for the optimizer.\n",
        "    :param momentum: Float. Momentum for SGD if optimizer=keras.optimizers.SGD\n",
        "    :param residual: Boolean. True if residual net. Otherwise 'plain' net.\n",
        "    :return: CNN with residual connections, similar to ResNet32\n",
        "    \"\"\"\n",
        "    \n",
        "    if len(layers_per_block) != len(n_filters) or len(layers_per_block) !=\\\n",
        "        len(kernel_sizes) or len(n_filters) != len(n_filters):\n",
        "        e = \"Length of 'layers_per_block', 'n_filters' and 'kernel_sizes'\" +\\\n",
        "        \" must be the same\"\n",
        "        raise Exception(e)  \n",
        "\n",
        "    for layers in layers_per_block:\n",
        "      if layers % 2 == 1:\n",
        "        e = \"Number of 'layers_per_block' must be even/pair numbers\"\n",
        "        raise Exception(e)\n",
        "      \n",
        "    inputs = Input(shape=img_size)\n",
        "    \n",
        "    n_filters_conv1 = 64\n",
        "    kernel_sizes_conv1 = (5,5) # kernel size for very first conv layer\n",
        "    strides_conv1 = (2,2)\n",
        "    \n",
        "    conv1 = Conv2D(n_filters_conv1, kernel_sizes_conv1, strides=strides_conv1,\n",
        "                   padding='same', kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                   kernel_initializer = glorot_uniform(0),\n",
        "                   bias_initializer = glorot_uniform(0))(inputs)\n",
        "    \n",
        "    layer_count = 1 # counter for the number of layers\n",
        "    for i in range(len(layers_per_block)):\n",
        "      for j in range(int(layers_per_block[i]/2)):\n",
        "        if j == 0:\n",
        "          if i == 0:\n",
        "            add = res_block(conv1, n_filters[i], kernel_sizes[i], l2_reg,\n",
        "                            residual, first=True)\n",
        "            #add = res_block(conv0_1, n_filters[i], kernel_sizes[i], l2_reg,\n",
        "            #                residual, subsampling=True)\n",
        "          else:\n",
        "            add = res_block(add, n_filters[i], kernel_sizes[i], l2_reg,\n",
        "                            residual, subsampling=True)\n",
        "        else:\n",
        "          add = res_block(add, n_filters[i], kernel_sizes[i], l2_reg,\n",
        "                          residual)\n",
        "          \n",
        "    bn = BatchNormalization()(add)\n",
        "    relu = Activation(\"relu\")(bn)\n",
        "\n",
        "    flat = GlobalAveragePooling2D()(relu)\n",
        "\n",
        "    out = Dense(n_classes, activation='softmax',\n",
        "                      kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                      kernel_initializer=glorot_uniform(0),\n",
        "                      bias_initializer=glorot_uniform(0))(flat)\n",
        "\n",
        "    res_cnn = Model(inputs=inputs, outputs=out)\n",
        "\n",
        "    if optimizer == keras.optimizers.Nadam:\n",
        "        res_cnn.compile(optimizer(lr=lr, schedule_decay=decay),\n",
        "                    \"categorical_crossentropy\", metrics=['accuracy'])\n",
        "    elif optimizer == keras.optimizers.SGD:\n",
        "        res_cnn.compile(optimizer(lr=lr, momentum=momentum, decay=decay),\n",
        "                        \"categorical_crossentropy\", metrics=['accuracy'])\n",
        "    else:\n",
        "        res_cnn.compile(optimizer(lr=lr, decay=decay),\n",
        "                        \"categorical_crossentropy\", metrics=['accuracy'])\n",
        "    return res_cnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m21vIzndbOIZ"
      },
      "source": [
        "### **Let's build a ResNet18:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rLvSCOTsAuKF",
        "outputId": "c37313ee-2607-4f97-c8b8-74dcaad64e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "classes = list(iter(train_generator.class_indices))\n",
        "n_classes = len(classes)\n",
        "layers_per_block = [4, 4, 4, 4] #18 layers total with first conv and last FC\n",
        "n_filters = [64, 128, 256, 512]\n",
        "kernel_sizes = [(3,3), (3,3), (3,3), (3,3)]\n",
        "l2_reg = 0.1\n",
        "optimizer = RMSprop # Adamax, RMSprop, Adam (No: Nadam, SGD)\n",
        "lr = 1e-3\n",
        "decay = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "res_cnn = make_resnet(img_size, n_classes, layers_per_block, n_filters,\n",
        "                       kernel_sizes, l2_reg, optimizer, lr, decay, momentum)\n",
        "res_cnn.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 128, 128, 64) 4864        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 128, 128, 64) 256         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 128, 128, 64) 0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPadding2D (None, 130, 130, 64) 0           activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 64)   0           zero_padding2d_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 64, 64, 64)   36928       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 64, 64, 64)   256         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 64, 64, 64)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 64, 64, 64)   4160        conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 64, 64, 64)   36928       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 64, 64, 64)   0           conv2d_24[0][0]                  \n",
            "                                                                 conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 64, 64, 64)   256         add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 64, 64, 64)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 64, 64, 64)   36928       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 64, 64, 64)   256         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 64, 64, 64)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 64, 64, 64)   36928       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 64, 64, 64)   0           add_9[0][0]                      \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 64, 64, 64)   256         add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 64, 64, 64)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 32, 32, 128)  73856       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 32, 32, 128)  512         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 32, 32, 128)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 32, 32, 128)  8320        add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 32, 32, 128)  147584      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 32, 32, 128)  0           conv2d_29[0][0]                  \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 32, 32, 128)  512         add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 32, 32, 128)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 32, 32, 128)  147584      activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 32, 32, 128)  512         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 32, 32, 128)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 32, 32, 128)  147584      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 32, 32, 128)  0           add_11[0][0]                     \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 32, 32, 128)  512         add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 32, 32, 128)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 16, 16, 256)  295168      activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 16, 16, 256)  1024        conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 16, 16, 256)  0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 16, 16, 256)  33024       add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 16, 16, 256)  590080      activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 16, 16, 256)  0           conv2d_34[0][0]                  \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 16, 16, 256)  1024        add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 16, 16, 256)  0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 16, 16, 256)  590080      activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 16, 16, 256)  1024        conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 16, 16, 256)  0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 16, 16, 256)  590080      activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 16, 16, 256)  0           add_13[0][0]                     \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 16, 16, 256)  1024        add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 16, 16, 256)  0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 512)    1180160     activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 512)    2048        conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 512)    0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 512)    131584      add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 512)    2359808     activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 8, 8, 512)    0           conv2d_39[0][0]                  \n",
            "                                                                 conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 512)    2048        add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 512)    0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 8, 8, 512)    2359808     activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 512)    2048        conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 512)    0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 8, 8, 512)    2359808     activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 8, 8, 512)    0           add_15[0][0]                     \n",
            "                                                                 conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 512)    2048        add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 512)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_2 (Glo (None, 512)          0           activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 3)            1539        global_average_pooling2d_2[0][0] \n",
            "==================================================================================================\n",
            "Total params: 11,188,419\n",
            "Trainable params: 11,180,611\n",
            "Non-trainable params: 7,808\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4GZSdBwZul7U"
      },
      "source": [
        "#### **Let's mount our GoogleDrive to download the best model (need to authenticate again):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YjYcj5zrumc5",
        "outputId": "c427a51f-1f03-4ae8-e720-c9ba44d9f86e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JUL3HGj4dyft"
      },
      "source": [
        "### **Let's train and validate our Base Model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N1-xGeCJ9jRe",
        "outputId": "4410ff56-18ec-49c1-8cff-01eb1ce6c1ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## fully balanced training\n",
        "## Rot 45,135,225,315\n",
        "## 'C4-7' vs 'C5' vs 'all_other' - Downsampling training\n",
        "\n",
        "## 128x128, stride_60,\n",
        "##min_pos_pix_1250, mivalpos_1024\n",
        "## ReduceLROnPlateau(monitor='val_loss'... )\n",
        "## train_batch_32, opt_RMSprop, Kernel_3x3:\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "train_steps = train_generator.n//train_generator.batch_size\n",
        "val_steps = val_generator.n//val_generator.batch_size\n",
        "\n",
        "# Callbacks:\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.85, patience=3, \n",
        "                                   verbose=1, mode='min', min_lr=1e-9)\n",
        "EarlyStop = EarlyStopping(monitor='val_acc', patience=70, verbose=1,\n",
        "                          min_delta=0, mode='max')\n",
        "checkpoint = ModelCheckpoint('base_model.h5', monitor='val_acc', verbose=1, \n",
        "                             save_best_only=True, mode='max')\n",
        "\n",
        "callbacks_list = [reduce_lr, checkpoint, EarlyStop] #order matters!\n",
        "\n",
        "#res_cnn.load_weights('base_model.h5')\n",
        "\n",
        "history = res_cnn.fit_generator(train_generator, steps_per_epoch=train_steps,\n",
        "                            validation_data=val_generator,\n",
        "                            validation_steps=val_steps, epochs=epochs,\n",
        "                            verbose=1, callbacks=callbacks_list, shuffle=False,\n",
        "                            class_weight=serial_pos_weights)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 9s 873ms/step - loss: 175.6311 - acc: 0.5250 - val_loss: 81.1778 - val_acc: 0.2333\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.23333, saving model to base_model.h5\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 1s 108ms/step - loss: 54.6237 - acc: 0.5706 - val_loss: 33.8478 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.23333 to 0.60000, saving model to base_model.h5\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 1s 95ms/step - loss: 24.9701 - acc: 0.5558 - val_loss: 17.3135 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.60000\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 13.5532 - acc: 0.6397 - val_loss: 10.4745 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.60000\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 1s 93ms/step - loss: 8.3140 - acc: 0.6586 - val_loss: 14.4329 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.60000\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 5.6532 - acc: 0.7090 - val_loss: 14.9282 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.60000\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 4.2859 - acc: 0.6554 - val_loss: 8.3256 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.60000\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 3.4420 - acc: 0.6198 - val_loss: 4.8995 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.60000\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 2.9565 - acc: 0.6598 - val_loss: 3.0675 - val_acc: 0.4333\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.60000\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 2.3668 - acc: 0.7415 - val_loss: 9.1877 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.60000\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 1s 87ms/step - loss: 2.2032 - acc: 0.6807 - val_loss: 11.7464 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.60000\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 1s 94ms/step - loss: 1.8341 - acc: 0.7438 - val_loss: 2.6094 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.60000\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 1s 95ms/step - loss: 1.8769 - acc: 0.6292 - val_loss: 2.4034 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.60000\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 1.5620 - acc: 0.7279 - val_loss: 2.1491 - val_acc: 0.4333\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.60000\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 1.4968 - acc: 0.7185 - val_loss: 2.0974 - val_acc: 0.5333\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.60000\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 1.5609 - acc: 0.7216 - val_loss: 2.4482 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.60000\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 1.3405 - acc: 0.7721 - val_loss: 2.0790 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.60000\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 1.4753 - acc: 0.7153 - val_loss: 2.5667 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.60000\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 1.2568 - acc: 0.7364 - val_loss: 2.7144 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.60000\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 1.3106 - acc: 0.7311 - val_loss: 1.8114 - val_acc: 0.4667\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.60000\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 1.2288 - acc: 0.7415 - val_loss: 2.4736 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.60000\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 1s 87ms/step - loss: 1.2068 - acc: 0.7248 - val_loss: 1.5913 - val_acc: 0.5333\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.60000\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 1s 95ms/step - loss: 1.0889 - acc: 0.7844 - val_loss: 2.5810 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.60000\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 1s 94ms/step - loss: 1.1920 - acc: 0.7345 - val_loss: 1.9739 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.60000\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 1.0878 - acc: 0.7721 - val_loss: 1.9606 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0008500000403728336.\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.60000\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 1s 94ms/step - loss: 1.0832 - acc: 0.7270 - val_loss: 2.8907 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.60000\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 1s 90ms/step - loss: 1.0211 - acc: 0.7815 - val_loss: 1.3500 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.60000\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.9572 - acc: 0.8162 - val_loss: 2.8901 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.60000\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 0.9436 - acc: 0.8361 - val_loss: 2.4650 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.60000\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 0.9978 - acc: 0.7658 - val_loss: 3.4445 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0007225000590551645.\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.60000\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 1s 90ms/step - loss: 0.9498 - acc: 0.7920 - val_loss: 2.2959 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.60000\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.8560 - acc: 0.8581 - val_loss: 3.3901 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.60000\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 1s 88ms/step - loss: 0.8896 - acc: 0.8266 - val_loss: 3.7620 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006141250254586339.\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.60000\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 1s 94ms/step - loss: 0.8753 - acc: 0.8281 - val_loss: 3.0239 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.60000\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 1s 95ms/step - loss: 0.8674 - acc: 0.8026 - val_loss: 2.6844 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.60000\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 1s 93ms/step - loss: 0.8731 - acc: 0.7932 - val_loss: 3.1577 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0005220062914304435.\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.60000\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.8691 - acc: 0.8026 - val_loss: 3.5052 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.60000\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 0.8671 - acc: 0.8121 - val_loss: 2.8826 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.60000\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.7740 - acc: 0.8572 - val_loss: 2.9974 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00044370535761117935.\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.60000\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.8262 - acc: 0.8572 - val_loss: 3.0166 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.60000\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 0.7940 - acc: 0.8257 - val_loss: 3.5934 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.60000\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.7912 - acc: 0.8487 - val_loss: 3.3518 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00037714955396950245.\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.60000\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 1s 90ms/step - loss: 0.8280 - acc: 0.8310 - val_loss: 2.8410 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.60000\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 1s 88ms/step - loss: 0.7145 - acc: 0.8645 - val_loss: 3.3282 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.60000\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 1s 96ms/step - loss: 0.7221 - acc: 0.8844 - val_loss: 3.1383 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0003205771208740771.\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.60000\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 1s 96ms/step - loss: 0.7095 - acc: 0.8739 - val_loss: 3.1711 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.60000\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 1s 99ms/step - loss: 0.6813 - acc: 0.8991 - val_loss: 3.0928 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.60000\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.6545 - acc: 0.9086 - val_loss: 2.7814 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0002724905527429655.\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.60000\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.6803 - acc: 0.8761 - val_loss: 2.4627 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.60000\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.6252 - acc: 0.9023 - val_loss: 2.3889 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.60000\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 0.8064 - acc: 0.8206 - val_loss: 2.3992 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.00023161696735769509.\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.60000\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 0.7078 - acc: 0.8666 - val_loss: 2.3235 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.60000\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.6258 - acc: 0.9117 - val_loss: 2.2879 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.60000\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 0.6556 - acc: 0.8720 - val_loss: 2.7077 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.00019687442472786642.\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.60000\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 1s 86ms/step - loss: 0.6189 - acc: 0.9149 - val_loss: 2.3478 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.60000\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 1s 96ms/step - loss: 0.6131 - acc: 0.9062 - val_loss: 2.4778 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.60000\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 1s 94ms/step - loss: 0.5893 - acc: 0.9307 - val_loss: 2.4104 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00016734325545257888.\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.60000\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 1s 90ms/step - loss: 0.6068 - acc: 0.8919 - val_loss: 2.3509 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.60000\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 1s 94ms/step - loss: 0.6178 - acc: 0.9054 - val_loss: 2.2086 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.60000\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 0.5703 - acc: 0.9401 - val_loss: 2.1827 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.00014224176775314845.\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.60000\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.6270 - acc: 0.8982 - val_loss: 1.8248 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.60000\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.5204 - acc: 0.9433 - val_loss: 1.9196 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.60000\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.5367 - acc: 0.9370 - val_loss: 2.1149 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.00012090550444554538.\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.60000\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 1s 91ms/step - loss: 0.6073 - acc: 0.8950 - val_loss: 1.9778 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.60000\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.5898 - acc: 0.9108 - val_loss: 1.9143 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.60000\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 1s 88ms/step - loss: 0.5934 - acc: 0.8814 - val_loss: 1.8197 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.00010276967877871357.\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.60000\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 1s 95ms/step - loss: 0.5711 - acc: 0.9344 - val_loss: 1.9450 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.60000\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 1s 95ms/step - loss: 0.5961 - acc: 0.8972 - val_loss: 1.9788 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.60000\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.5408 - acc: 0.9307 - val_loss: 1.9756 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00069: ReduceLROnPlateau reducing learning rate to 8.735422634345013e-05.\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.60000\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.6901 - acc: 0.8814 - val_loss: 1.8597 - val_acc: 0.4667\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.60000\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.5541 - acc: 0.9307 - val_loss: 1.9118 - val_acc: 0.4333\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.60000\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 1s 92ms/step - loss: 0.5038 - acc: 0.9464 - val_loss: 1.7255 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00072: ReduceLROnPlateau reducing learning rate to 7.425108960887882e-05.\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.60000\n",
            "Epoch 00072: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E5mhZnDUvLsL"
      },
      "source": [
        "#### **Let's download the best model to our 'Capstone' folder in GoogleDrive:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a4nSrCq4vQCi",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  files.download('base_model.h5')\n",
        "except:\n",
        "  print(\"Not done\")\n",
        "  pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MRz_1oJzJRFH"
      },
      "source": [
        "#### **Let's download the training history to a local file (just in case colab's session is interrupted):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k5Y5GUUhcyAt",
        "colab": {}
      },
      "source": [
        "for k,v in history.history.items():\n",
        "  history.history[k] = str(v)\n",
        "\n",
        "with open('history_dict.json', 'w') as f:\n",
        "    json.dump(history.history, f)\n",
        "\n",
        "try:\n",
        "    time.sleep(3) # To avoid warning when downloading various files at once\n",
        "    files.download('history_dict.json')\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jxEpvYPVcyUb",
        "colab": {}
      },
      "source": [
        "#uploaded = files.upload()\n",
        "with open('history_dict.json') as f:\n",
        "    history_dict = json.load(f)\n",
        "    \n",
        "for k,v in history_dict.items():\n",
        "  history_dict[k] = json.loads(v)\n",
        "\n",
        "history_df = pd.DataFrame(history_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_eAC27OxKOLY"
      },
      "source": [
        "#### **Let's plot training and validation loss vs epochs:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DCtryy1abrRA",
        "outputId": "6a914d65-ca28-4810-9c49-e6a56195fc0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "loss = history_df[['loss','val_loss']]\n",
        "loss.columns = ['train_loss', 'val_loss']\n",
        "loss.plot(figsize=(10, 6), title='Loss vs epochs')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f55fe6386a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAF1CAYAAAAqdaQaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VSFAqg-gKlR0"
      },
      "source": [
        "#### **Let's plot training and validation accuracy vs epochs:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bAbt9je3b1x1",
        "outputId": "68fff24c-a702-4dcb-ebd9-563e17e075b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "acc = history_df[['val_acc','acc']]\n",
        "acc.columns = ['val_acc', 'train_acc']\n",
        "acc.plot(figsize=(10, 6), title='Val acc & Train acc vs epochs')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f55fe0f17b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF1CAYAAADMXG9eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B8RiTO3yLDXy"
      },
      "source": [
        "#### **Let's plot learning rate vs epochs:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o2ggKlccOAHA",
        "outputId": "7730a919-364e-405c-e38d-644aa62fd850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "lr = history_df[['lr']]\n",
        "lr.plot(figsize=(10, 6), title='Learning rate vs epochs')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f55fe0044e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAF1CAYAAABChiYiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deZxeZX3//9cnM5NMkskkYbKRPSSB\nMGELhC24gIiCWlKqIogWWhRtRf3aFaqt1kdtpf7qVqUuoMWNRSoacasUcWEJTALKGhhCQgKB7DtZ\nZnL9/rhPdBxnyz3Lue97Xs/HI4+c+yzX/TknmTzeOee6rhMpJSRJkpS/IXkXIEmSpAKDmSRJUokw\nmEmSJJUIg5kkSVKJMJhJkiSVCIOZJElSiTCYScpdRPwoIi7Nu47BLCL+OyL+Je86pMHOYCYNYhGx\nKiJenXcdKaXzUko35F0HQETcFRHvyLsOSYOTwUxSv4qI6rxrOKiUapGkjhjMJHUoIt4QEQ9FxNaI\nuCcijmuz7aqIeDoidkTEYxFxQZttl0XE3RHxqYjYBHwkW/eriPj/ImJLRDwTEee1Oea3d6l6sO+s\niPhF9t13RMTnI+IbnZzDmRGxNiL+PiJeAL4aEWMj4vaI2JC1f3tETM32/xjwcuBzEbEzIj6XrZ8X\nET+NiM0RsSIiLuzk+94SEU3t1n0gIpZky6/LrteOiHguIv6mi+v/5xHxeFbjTyJiRpttKSLeFxEr\nI2JjRHwiIoZk24ZExIciYnVErI+Ir0XE6DbHviz789waEWsi4rI2Xzs2In6Q1bc0ImZnx0T257k+\nIrZHxMMRcUxntUsqnsFM0h+IiAXAV4B3AQ3AF4ElETEs2+VpCgFmNPDPwDci4vA2TZwKrAQmAh9r\ns24FMA74d+D6iIhOSuhq328B92d1fQR4ezenMwk4DJgBXEHh372vZp+nAy8BnwNIKX0Q+CVwZUqp\nLqV0ZUSMBH6afe8E4CLg2oho7OC7vg8cFRFz26x7a3YswPXAu1JKo4BjgDs7KjgiFgP/APwJMD6r\n6cZ2u10ALAROBBYDf56tvyz7dRZwBFB38PyycPcj4D+zdk8AHmrT5kUU/jzHAs387s/uNcArgCMp\n/JlfCGzqqHZJvWMwk9SRK4AvppSWppRas/5fe4HTAFJK304pPZ9SOpBSuhl4CjilzfHPp5T+M6XU\nklJ6KVu3OqX05ZRSK3ADcDiF4NaRDveNiOnAycA/pZT2pZR+BSzp5lwOAB9OKe1NKb2UUtqUUvqf\nlNLulNIOCuHjlV0c/wZgVUrpq9n5PAj8D/Dm9jumlHYD3wMuBsgC2rw2Ne4HGiOiPqW0JaW0vJPv\nfDfwbymlx1NKLcC/Aie0vWsGXJNS2pxSehb49MHvBC4BPplSWplS2glcDVyUPcZ9K3BHSunGlNL+\n7Fq0DWa3pZTuz77zmxSC28G6R2XnElld67q4ZpKKZDCT1JEZwF9nj7u2RsRWYBowGSAi/rTNY86t\nFO7+jGtz/JoO2nzh4EIWYKBwN6cjne07GdjcZl1n39XWhpTSnoMfImJERHwxe9S3HfgFMCYiqjo5\nfgZwartrcQmFO3Ed+Ra/C0lvBb7bpt43Aq8DVkfEzyPi9C6+8zNtvm8zEMCUNvu0Pe/VZH822e+r\n222rphCCp1G429mZF9os7yb780kp3UnhrtvngfUR8aWIqO+iHUlFMphJ6sga4GMppTFtfo1IKd2Y\n3bX5MnAl0JBSGgM8QiE4HJT6qa51wGERMaLNumndHNO+lr8GjgJOTSnVU3hEB7+rv/3+a4Cft7sW\ndSmlv+jk+34KjI+IEygEtIOPMUkpPZBSWkzhkeh3gVs6aWMNhUeebb9zeErpnjb7tD3v6cDz2fLz\nFIJd220twItZu7M7+c4upZQ+m1I6CWik8Ejzb4tpR1LXDGaSaiKits2vagrB690RcWrW8XtkRLw+\nIkYBIymElw0AEfFnFO6Y9buU0mqgicKAgqHZHac/OsRmRlHoV7Y1Ig4DPtxu+4sU+mYddDtwZES8\nPSJqsl8nR8TRndS4H/g28AkKfdt+CpDVe0lEjM722U7hMWtHvgBcHRHzs2NHR0T7R6d/G4WBDNOA\n9wM3Z+tvBD4QhUESdRQeg97c5vHkqyPiwoiojoiGLEB2KTvfUyOiBtgF7Omidkm9YDCT9EMKQeXg\nr4+klJqAd1J4fLWFQkfwywBSSo8B/wHcSyHEHAvcPYD1XgKcTqHz+b9QCCR7D+H4TwPDgY3AfcCP\n223/DPCmbDTkZ7N+aK+h0DH+eQqP+64BhtG5bwGvBr6dBaKD3g6syh6hvjs7lz+QUrot+46bsn0f\nAc5rt9v3gGUUOu//gMLAAigM2vg6hUe0z1AIUe/N2n2WwqPUv6bwePQh4PguzuOgegphfQuFR6Ob\nKARPSX0sUuqvJw6S1P8i4mbgiZRS+ztfFSsiEjA3pdScdy2S+pZ3zCSVleyx2uxsvq5zKUwV8d28\n65KkvuAs2JLKzSTgOxTmMVsL/EU2hYUklT0fZUqSJJUIH2VKkiSVCIOZJElSiaiIPmbjxo1LM2fO\nzLsMSZKkbi1btmxjSml8R9sqIpjNnDmTpqamvMuQJEnqVkSs7mybjzIlSZJKhMFMkiSpRBjMJEmS\nSkRF9DGTJEmDx/79+1m7di179uzJu5Qu1dbWMnXqVGpqanp8jMFMkiSVlbVr1zJq1ChmzpxJRORd\nTodSSmzatIm1a9cya9asHh/no0xJklRW9uzZQ0NDQ8mGMoCIoKGh4ZDv6hnMJElS2SnlUHZQMTUa\nzCRJkg5RXV1dv7RrMJMkSeoDLS0tvW6jR8EsIs6NiBUR0RwRV3WwfVhE3JxtXxoRM9tsuzpbvyIi\nXttm/VciYn1EPNKurcMi4qcR8VT2+9jiT0+SJKn/3HXXXbz85S/n/PPPp7GxsdftdTsqMyKqgM8D\n5wBrgQciYklK6bE2u10ObEkpzYmIi4BrgLdERCNwETAfmAzcERFHppRagf8GPgd8rd1XXgX8X0rp\n41kIvAr4+96cpCRJqkz//P1Heez57X3aZuPkej78R/N7vP/y5ct55JFHDmn0ZWd6Ml3GKUBzSmkl\nQETcBCwG2gazxcBHsuVbgc9FocfbYuCmlNJe4JmIaM7auzel9Iu2d9batXVmtnwDcBfdBLMde1r4\n2Yr1PTgVqfzV11Zz0ozD8i5DkpQ55ZRT+iSUQc+C2RRgTZvPa4FTO9snpdQSEduAhmz9fe2OndLN\n901MKa3Lll8AJna0U0RcAVwBMHTSHP7sqw90fyZShfjh+15O4+T6vMuQpNwdyp2t/jJy5Mg+a6uk\nJ5hNKaWISJ1s+xLwJYDG4xakb/7logGtTcrDi9v38O5vLOfxddsNZpJUgXoSzJ4DprX5PDVb19E+\nayOiGhgNbOrhse29GBGHp5TWRcThQLfPKEcMrWLBdMcIqPK1tB6gpip4av3OvEuRJPWDnozKfACY\nGxGzImIohc78S9rtswS4NFt+E3BnSill6y/KRm3OAuYC93fzfW3buhT4Xg9qlAaF6qohzGwYSbPB\nTJJytXNn4d/hM888k9tvv73P2u02mKWUWoArgZ8AjwO3pJQejYiPRsT52W7XAw1Z5/6/ojCSkpTS\no8AtFAYK/Bh4TzYik4i4EbgXOCoi1kbE5VlbHwfOiYingFdnnyVl5k6s4+kNBjNJqkQ96mOWUvoh\n8MN26/6pzfIe4M2dHPsx4GMdrL+4k/03AWf3pC5pMJozvo4fP/ICe/a3UltTlXc5kqQ+5Mz/UpmZ\nPaGOAwlWbdqVdymSpD5mMJPKzNwJowDsZyZpUCt0ZS9txdRoMJPKzBHjRxJhMJM0eNXW1rJp06aS\nDmcpJTZt2kRtbe0hHVfS85hJ+kO1NVVMGzvCKTMkDVpTp05l7dq1bNiwIe9SulRbW8vUqVMP6RiD\nmVSG5k6o42mDmaRBqqamps9egVRqfJQplaE5E+pYuXEXLa0H8i5FktSHDGZSGZo9oY59LQdYs+Wl\nvEuRJPUhg5lUhuZMqAMcACBJlcZgJpUhg5kkVSaDmVSG6mtrmFg/zGAmSRXGYCaVqTkT6mhevyPv\nMiRJfchgJpWpuRNG8fSGXSU9waIk6dAYzKQyNXtCHTv3tvDC9j15lyJJ6iMGM6lMzRlfGADw1Iv2\nM5OkSmEwk8qUIzMlqfIYzKQyNa5uKGNG1NC8wWAmSZXCYCaVqYhgzvg675hJUgUxmEllrDBlhsFM\nkiqFwUwqY3Mm1LF51z4279qXdymSpD5gMJPKmAMAJKmyGMykMnYwmD3lGwAkqSIYzKQyNnn0cIbX\nVHnHTJIqhMFMKmNDhoQDACSpghjMpDI3Z0IdTxvMJKkiGMykMjdnQh3Pb9vDzr0teZciSeolg5lU\n5g4OAPCumSSVP4OZVOacMkOSKofBTCpzMw4bQU1V8JTBTJLKnsFMKnPVVUOY2TDSO2aSVAEMZlIF\nmDuxjqc3GMwkqdwZzKQKMGd8Has37WLP/ta8S5Ek9YLBTKoAsyfUcSDBqk278i5FktQLBjOpAsyd\nMApwZKYklTuDmVQBjhg/kgiDmSSVO4OZVAFqa6qYNnaEU2ZIUpkzmEkVwndmSlL5M5hJFWLuhDpW\nbtxFS+uBvEuRJBXJYCZViNkT6tjXcoA1W17KuxRJUpEMZlKF8J2ZklT+DGZShTCYSVL5M5hJFaK+\ntoaJ9cMMZpJUxgxmUgWZM6GO5vU78i5DklSk6rwLkNR35oyv45amtfzL7Y/l8v1DhgSXnDqdGQ0j\nc/l+SSp3BjOpgrziyPHc9uBz3Hj/s7l8/659rbS0Jv7pjxpz+X5JKncGM6mCnH30RH7zkdfm9v0X\nXHs3jz6/Lbfvl6RyZx8zSX1m/uR6Hlu3nZRS3qVIUlkymEnqM42Hj2bHnhbWOsmtJBXFYCapz8yf\nXA/g40xJKpLBTFKfOWrSKKqGBI89vz3vUiSpLBnMJPWZ2poqZo8fyaMGM0kqisFMUp9qPLwwAECS\ndOgMZpL61PzJo1m3bQ+bd+3LuxRJKjsGM0l9qtEBAJJUNIOZpD51cGSmAwAk6dAZzCT1qTEjhjJl\nzHAHAEhSEQxmkvrc0Q4AkKSi9CiYRcS5EbEiIpoj4qoOtg+LiJuz7UsjYmabbVdn61dExGu7azMi\nzo6I5RHxUET8KiLm9O4UJQ20+ZPrWblhJy/ta827FEkqK90Gs4ioAj4PnAc0AhdHRGO73S4HtqSU\n5gCfAq7Jjm0ELgLmA+cC10ZEVTdt/hdwSUrpBOBbwId6d4qSBlrj5HoOJHjiBe+aSdKh6Mkds1OA\n5pTSypTSPuAmYHG7fRYDN2TLtwJnR0Rk629KKe1NKT0DNGftddVmAuqz5dHA88WdmqS8/O7VTAYz\nSToU1T3YZwqwps3ntcCpne2TUmqJiG1AQ7b+vnbHTsmWO2vzHcAPI+IlYDtwWkdFRcQVwBUA06dP\n78FpSBooU8YMZ/TwGoOZJB2iUuz8/wHgdSmlqcBXgU92tFNK6UsppYUppYXjx48f0AIldS0ifAOA\nJBWhJ8HsOWBam89Ts3Ud7hMR1RQeQW7q4tgO10fEeOD4lNLSbP3NwKIenYmkkjJ/cj1PrNtOS+uB\nvEuRpLLRk2D2ADA3ImZFxFAKnfmXtNtnCXBptvwm4M6UUsrWX5SN2pwFzAXu76LNLcDoiDgya+sc\n4PHiT09SXhon17O35QDPbNyVdymSVDa67WOW9Rm7EvgJUAV8JaX0aER8FGhKKS0Brge+HhHNwGYK\nQYtsv1uAx4AW4D0ppVaAjtrM1r8T+J+IOEAhqP15n56xpAExf/JooDAAYO7EUTlXI0nlIQo3tsrb\nwoULU1NTU95lSGpjf+sB5n/4J1x6+gw++Pr2M+xI0uAVEctSSgs72laKnf8lVYCaqiHMmzTKAQCS\ndAgMZpL6TePh9Tz6/HYq4c68JA0Eg5mkfjN/cj1bd+9n3bY9eZciSWXBYCap3zT6BgBJOiQGM0n9\nZt6keiLg0ee35V2KJJUFg5mkfjNyWDWzxo3kMe+YSVKPGMwk9auDAwAkSd0zmEnqV/Mnj+a5rS+x\nbff+vEuRpJJnMJPUr347AGCd/cwkqTsGM0n9qvHwQjCzn5kkdc9gJqlfjR81jAmjhhnMJKkHDGaS\n+t38yQ4AkKSeMJhJ6nfzJ4+mecNO9uxvzbsUSSppBjNJ/a5xcj2tBxJPvrgj71IkqaQZzCT1u/mT\nHQAgST1hMJPU76aNHUHdsGr7mUlSNwxmkvrdkCGRvQHAucwkqSsGM0kDonFyPU+8sIPWAynvUiSp\nZFXnXYCkwaFxcj2797Vy/zObmTp2eFFt1NfWMHpETR9XJkmlw2AmaUAcN3U0ABd/+b6i2xheU8V9\nV59tOJNUsQxmkgbEvEn1XPenC9n6UnEvM3920y4+e2czy9ds4ayjJvRxdZJUGgxmkgbMqxsnFn3s\nrr0tfO5nzTy42mAmqXLZ+V9SWRg5rJp5k+pZ/uzWvEuRpH5jMJNUNk6cMYaH1mx1ZKekimUwk1Q2\nTpw+lp17W3hqva92klSZDGaSysaJ08cCsHy1jzMlVSaDmaSyMaNhBIeNHMryZ7fkXYok9QuDmaSy\nERGcOH2MwUxSxTKYSSorC6aPZeWGXWzZtS/vUiSpzxnMJJWVk2YU+pk9uMa7ZpIqj8FMUlk5bupo\nqoaEAwAkVSSDmaSyMmJoNUcfPsp+ZpIqksFMUtk5cfpYfu1Es5IqkMFMUtk5cfpYdu1rZcULTjQr\nqbIYzCSVnd9ONOvjTEkVxmAmqexMO2w44+qcaFZS5TGYSSo7EcGC6WNZvtpgJqmyGMwklaWTZoxl\n1abdbNq5N+9SJKnPGMwklaWD/cwefNb5zCRVDoOZpLJ03NTRVA8J+5lJqigGM0llqbamisbJ9QYz\nSRXFYCapbBUmmt1GS+uBvEuRpD5hMJNUthZMH8NL+1t5wolmJVUIg5mksvW7AQA+zpRUGQxmksrW\n1LHDGT9qGMucz0xShTCYSSpbEcFJ08ey3CkzJFUIg5mksnbijDE8u3k3G51oVlIFMJhJKmu/faG5\njzMlVQCDmaSydsyU0dRUhY8zJVUEg5mkslaYaHa0E81KqggGM0ll78TpY/jN2q3sd6JZSWXOYCap\n7J04fSx79h/giXVONCupvBnMJJW9E2cUBgAsW70550okqXcMZpLK3uTRtUyqr3UAgKSyV513AZLU\nWxHBiTPG8IOH1/Gr5o1Ft/OuVxzBu145uw8rk6RDYzCTVBH+8sw5jKsbRkrFHX9380ZufmCNwUxS\nrnoUzCLiXOAzQBVwXUrp4+22DwO+BpwEbALeklJalW27GrgcaAXel1L6SVdtRkQA/wK8OTvmv1JK\nn+3daUqqdMdMGc0xU0YXffx1v1zJv/zgcdZte4nDRw/vw8okqee67WMWEVXA54HzgEbg4ohobLfb\n5cCWlNIc4FPANdmxjcBFwHzgXODaiKjqps3LgGnAvJTS0cBNvTpDSeqBRbPHAXB386acK5E0mPWk\n8/8pQHNKaWVKaR+FoLS43T6LgRuy5VuBs7M7X4uBm1JKe1NKzwDNWXtdtfkXwEdTSgcAUkrriz89\nSeqZeZNGcdjIodzTiz5qktRbPQlmU4A1bT6vzdZ1uE9KqQXYBjR0cWxXbc4G3hIRTRHxo4iY21FR\nEXFFtk/Thg0benAaktS5IUOC02c3cPfTG0nFdlSTpF4qxekyhgF7UkoLgS8DX+lop5TSl1JKC1NK\nC8ePHz+gBUqqTC+bM44Xt+/l6Q078y5F0iDVk2D2HIU+XwdNzdZ1uE9EVAOjKQwC6OzYrtpcC3wn\nW74NOK4HNUpSr51hPzNJOetJMHsAmBsRsyJiKIXO/Eva7bMEuDRbfhNwZyo8C1gCXBQRwyJiFjAX\nuL+bNr8LnJUtvxJ4srhTk6RDM71hBFPHDudu+5lJykm302WklFoi4krgJxSmtvhKSunRiPgo0JRS\nWgJcD3w9IpqBzRSCFtl+twCPAS3Ae1JKrQAdtZl95ceBb0bEB4CdwDv67nQlqWtnzB7HDx9ZR+uB\nRNWQyLscSYNMVEIn14ULF6ampqa8y5BUAZb8+nned+ODfPc9Z3DCtDF5lyOpAkXEsqwv/R8oxc7/\nkpSbRbMbAHycKSkXBjNJamNc3TDmTRrFPU8bzCQNPIOZJLWzaPY4mlZtYc/+1rxLkTTIGMwkqZ0z\n5jSwt+UAy1dvybsUSYOMwUyS2jn1iAaqhgS/sp+ZpAFmMJOkduqGVXPCtDHc/bQTzUoaWAYzSerA\nGbMbeHjtVra9tD/vUiQNIgYzSerAojnjOJBg6UrvmkkaOAYzSerAguljGF5TxT0+zpQ0gAxmktSB\nYdVVnDzrMAcASBpQBjNJ6sQZsxtoXr+TF7fvybsUSYOEwUySOnHGnHEAvgVA0oAxmElSJxoPr2fM\niBrubrafmaSBYTCTpE4MGRIsmt3APc0bSSnlXY6kQcBgJkldWDR7HM9v28MzG3flXYqkQcBgJkld\nONjPzLcASBoIBjNJ6sLMhhFMHl3LPU6bIWkAGMwkqQsRwaI547h35SZaD9jPTFL/MphJUjdeNmcc\nW3fv57Hnt+ddiqQKV513AZJU6hbNbgDg/M//iiiyjZFDq/nelWdwxPi6vitMUsUxmElSNybU1/If\nbz6eVZuKG5nZeiBx7V1Pc/tv1vG+s+f2cXWSKonBTJJ64I0nTe3V8fc8vYmfPvaiwUxSl+xjJkkD\n4JzGiTz83DbWbXsp71IklTCDmSQNgNfOnwjAHY+9mHMlkkqZwUySBsDs8XXMGjeS/zWYSeqCwUyS\nBkBEcE7jRO5buYnte/bnXY6kEmUwk6QBck7jRPa3Jn6+YkPepUgqUQYzSRogJ04fS8PIoT7OlNQp\ng5kkDZCqIcGr5k3grifWs6/lQN7lSCpBBjNJGkDnNE5kx94Wlj6zKe9SJJUgg5kkDaCXzx1Pbc0Q\nfurjTEkdMJhJ0gAaPrSKl88dzx2PvUhKKe9yJJUYg5kkDbBzGify/LY9PPr89rxLkVRiDGaSNMDO\nnjeBIYGjMyX9AYOZJA2whrphnDRjrP3MJP0Bg5kk5eCcxok8vm47azbvzrsUSSXEYCZJOTincRIA\ndzzuXTNJv2Mwk6QczBo3kjkT6nycKen3GMwkKSfnNE5k6TOb2bbbl5pLKjCYSVJOzmmcSOuBxJ0r\nvGsmqcBgJkk5OWHqGMaPGubjTEm/ZTCTpJwMGRK8+uiJ/HzFBva2tOZdjqQSYDCTpBy9pnEiu/a1\ncs/TvtRcksFMknJ1+uwGRgyt8nGmJACq8y5Akgaz2poqXnnkeH78yAuMqKkqup3Tjmjg1Y0T+7Ay\nSXkwmElSzi5cOI27mzdy4/3PFnX8vtYDfHvZWu7/4NkMqy4+3EnKn8FMknJ21rwJ/OYjry36+LtW\nrOeyrz7Az55Yz7nHHN6HlUkaaPYxk6Qy97I54xhXN4zvLH8u71Ik9ZLBTJLKXHXVEBafMJmfrVjP\nll378i5HUi8YzCSpAlywYAr7WxO3P7wu71Ik9YLBTJIqwPzJ9Rw5sY7blq/NuxRJvWAwk6QKEBH8\nyYlTWf7sVlZt3JV3OZKKZDCTpAqx+ITJRMBtDzoIQCpXBjNJqhCHjx7OotkNfPeh50gp5V2OpCIY\nzCSpglywYCqrN+1m+bNb8i5FUhEMZpJUQc49ZhK1NUOc00wqUz0KZhFxbkSsiIjmiLiqg+3DIuLm\nbPvSiJjZZtvV2foVEfHaQ2jzsxGxs7jTkqTBqW5YNa+dP4nbf7OOvS2teZcj6RB1G8wiogr4PHAe\n0AhcHBGN7Xa7HNiSUpoDfAq4Jju2EbgImA+cC1wbEVXdtRkRC4GxvTw3SRqULlgwhW0v7ednT2zI\nuxRJh6gnd8xOAZpTSitTSvuAm4DF7fZZDNyQLd8KnB0Rka2/KaW0N6X0DNCctddpm1lo+wTwd707\nNUkanA6+oum2B53TTCo3PQlmU4A1bT6vzdZ1uE9KqQXYBjR0cWxXbV4JLEkpdTl9dURcERFNEdG0\nYYP/K5Skgw6+ounOJ9azdbevaJLKSUl1/o+IycCbgf/sbt+U0pdSSgtTSgvHjx/f/8VJUhn57Sua\nfuMrmqRy0pNg9hwwrc3nqdm6DveJiGpgNLCpi2M7W78AmAM0R8QqYERENPfwXCRJmd++osnJZqWy\n0pNg9gAwNyJmRcRQCp35l7TbZwlwabb8JuDOVJjdcAlwUTZqcxYwF7i/szZTSj9IKU1KKc1MKc0E\ndmcDCiRJhyAiuGDBVJat3sLqTb6iSSoX3QazrM/YlcBPgMeBW1JKj0bERyPi/Gy364GG7O7WXwFX\nZcc+CtwCPAb8GHhPSqm1szb79tQkaXD74wW+okkqN1EJr+1YuHBhampqyrsMSSo5b/3yfTy39SXu\n+pszKQyWl5S3iFiWUlrY0baS6vwvSepbFyyY4iuapDJSnXcBkqT+c96xh/OP33uEt113P8Nqivu/\nePWQIXz2ohNYNGdcH1cnqT2DmSRVsLph1fz7m45n2arNRbfxg4df4Nq7njaYSQPAYCZJFe784ydz\n/vGTiz5+Qn0tn/jJCp56cQdzJ47qw8oktWcfM0lSly4+ZTpDq4dww72r8i5FqngGM0lSlw4bOZTF\nx0/mf5Y9x7aX9uddjlTRDGaSpG5dumgmL+1v5dtNa7rfWVLRDGaSpG4dM2U0C2eM5Wv3rqb1QPnP\nfymVKoOZJKlHLjtjJs9u3s1dK9bnXYpUsQxmkqQeee38SUyqr+W/71mVdylSxTKYSZJ6pKZqCG87\nbTq/fGojzet35l2OVJEMZpKkHrvolOkMrRrC1+5dlXcpUkUymEmSemxc3TD+6PjJ3LpsLdv3OHWG\n1NcMZpKkQ3LZopns3tfKrU1r8y5FqjgGM0nSITl26mhOmjGWr927igNOnSH1KYOZJOmQXbpoJqs2\n7ebnT27IuxSpohjMJEmH7LxjJjGxfhhfdeoMqU8ZzCRJh6ymagiXnDqDXzy5gac3OHWG1FcMZpKk\nolycTZ3x9XtX512KVDGq8y5AklSexo8axhuOO5xvN62hpiqKbmf2+DouOmV6H1YmlS+DmSSpaO98\nxRHc9eQGvrn02aKObz2Q2LYSv9EAAA/YSURBVNtygGOmjOaYKaP7uDqp/ERK5T/UeeHChampqSnv\nMiRJh2j7nv28/JqfcfLMsVx36cl5lyMNiIhYllJa2NE2+5hJknJTX1vDO142izseX89v1m7Nuxwp\ndwYzSVKuLjtjJmNG1PDpO57KuxQpdwYzSVKuRtXW8M6XH8GdT6znoTXeNdPgZjCTJOXu0kUH75o9\nmXcpUq4MZpKk3NUNq+adLz+Cu1Zs4MFnt+RdjpQbg5kkqSRcumgmY+1rpkHOYCZJKgl1w6q54hWz\n+fmTG1i22rtmGpwMZpKkkvGnp8/gsJFD7WumQctgJkkqGSOHVfOuVxzBL5/ayLLVm/MuRxpwBjNJ\nUkl5++kzaBg51L5mGpQMZpKkkjJiaDXvemXhrlnTKu+aaXAxmEmSSs7bTpvBuLqhfMq+ZhpkDGaS\npJIzYmg1737lbO5u3sT9z3jXTINHdd4FSJLUkUtOncEXfr6St1+/lOFDq4pqoyqCvz93HheePK2P\nq5P6h8FMklSShg+t4jMXncD/PvpC0W0sf3YrH/n+o7ziyPFMGl3bh9VJ/cNgJkkqWWfMGccZc8YV\nffyzm3bz6k/9nH/70eN85qIFfViZ1D/sYyZJqljTG0bw7lccwfceep6lKzflXY7ULYOZJKmi/cWZ\nc5gyZjgfXvIoLa0H8i5H6pLBTJJU0YYPreJDrz+aJ17YwTeXPpt3OVKXDGaSpIp37jGTOGNOA//x\nvyvYtHNv3uVInTKYSZIqXkTwkT+az+59rXziJyvyLkfqlMFMkjQozJ04issWzeTmpjX8es3WvMuR\nOmQwkyQNGu9/9VwaRg7jn5Y8yoEDKe9ypD9gMJMkDRqjamu4+rx5/HrNVm5dvjbvcqQ/YDCTJA0q\nFyyYwkkzxnLNj55g20v78y5H+j0GM0nSoDJkSPDP589n8+59fPqOJ/MuR/o9vpJJkjToHDNlNG89\nZTpfu3c1G3fuY0gU186k+lo+cM6R1NYU95J1qT2DmSRpUPqb1xzFMxt38fDa4kZoJmD1pt0cSIkP\nvr6xb4vToGUwkyQNSmNHDuVb7zytV2186LsPc92vnuGseRNYNLv4l61LB9nHTJKkIv3D645mVsNI\n/uaWXzuQQH3CYCZJUpFGDK3mk285gRd37OUjSx7NuxxVAIOZJEm9cMK0Mbz3VXO47cHnuP03z+dd\njsqcwUySpF56z1lzOH7aGD542yO8sG1P3uWojBnMJEnqpZqqIXzqwuPZ13KAv731177uSUXrUTCL\niHMjYkVENEfEVR1sHxYRN2fbl0bEzDbbrs7Wr4iI13bXZkR8M1v/SER8JSJqeneKkiT1vyPG1/HB\n1x/NL5/ayNfvW513OSpT3QaziKgCPg+cBzQCF0dE+wlbLge2pJTmAJ8CrsmObQQuAuYD5wLXRkRV\nN21+E5gHHAsMB97RqzOUJGmAXHLqdM46ajz/+sPHaV6/M+9yVIZ6csfsFKA5pbQypbQPuAlY3G6f\nxcAN2fKtwNkREdn6m1JKe1NKzwDNWXudtplS+mHKAPcDU3t3ipIkDYyI4Jo3HceIoVV84OaH2N96\nIO+SVGZ6MsHsFGBNm89rgVM72yel1BIR24CGbP197Y6dki132Wb2CPPtwPs7KioirgCuAJg+fXoP\nTkOSpP43YVQt//Ynx/LubyznQ7c9wmmzDyu6rVNnNTB5zPA+rE6lrpRn/r8W+EVK6ZcdbUwpfQn4\nEsDChQvtZSlJKhnnHnM4F58yjRvvX8PNTWu6P6ATE0YN4/vvfRkT62v7sDqVsp4Es+eAaW0+T83W\ndbTP2oioBkYDm7o5ttM2I+LDwHjgXT2oT5KkkvOvFxzLX545h9YiR2iu27aHy294gCu+voybrzjN\nF6UPEj0JZg8AcyNiFoXwdBHw1nb7LAEuBe4F3gTcmVJKEbEE+FZEfBKYDMyl0G8sOmszIt4BvBY4\nO6Xkw3lJUlmKCKYdNqLo42eOG8knLzyBd39jGf/wnYf5jwuPp9B9W5Ws287/KaUW4ErgJ8DjwC0p\npUcj4qMRcX622/VAQ0Q0A38FXJUd+yhwC/AY8GPgPSml1s7azNr6AjARuDciHoqIf+qjc5Ukqayc\ne8wk/uqcI/nOg8/x5V+uzLscDYAoDH4sbwsXLkxNTU15lyFJUp9LKXHltx7kR4+s4/rLTuasoybk\nXZJ6KSKWpZQWdrTNmf8lSSphEcEn3nwc8ybV875vPej8aBXOYCZJUokbMbSaL/3pSQytHsIVX2ti\n20v78y5J/cRgJklSGZg6dgRfePtJrNmym/fe+GDRoz1V2gxmkiSViZNnHsZHFx/DL57cwMd/9Hje\n5agflPIEs5IkqZ2LT5nOE+u28+VfPkPT6i1UDyluCo0RQ6v54OuP5siJo/q4QvWGwUySpDLzoTc0\nEhE8+eKOott4+LltvPXLS7nlXadxxPi6PqxOveF0GZIkDULN63fwli/ex9DqIdzyrtN7NRmuDo3T\nZUiSpN8zZ8Iovn75qeze18rFX76PddteyrskYTCTJGnQapxcz9cvP4Vtu/dzyZeXsn7HnrxLGvQM\nZpIkDWLHTR3DV//sZF7Yvoe3XbeUzbv25V3SoGYwkyRpkFs48zCuu3Qhqzft5m3XLWXbbiewzYvB\nTJIksWj2OL749pNoXr+TS796Pzv3tuRd0qDkdBmSJAmAM4+awOfeuoC//OZyLvzCvRw7ZXTRbR0z\npZ63nTaDiOLmWRusDGaSJOm3XjN/Ep+9eAEf/9ET/PzJDUW10XIgcXPTGn6zdhv/+ifHUlPlA7qe\nMphJkqTf87pjD+d1xx5e9PEpJT59x1N85v+e4sUde7n2khOpG2bk6AkjrCRJ6lMRwQfOOZJr3ngs\ndzdv5C1fvJf1252KoycMZpIkqV+85eTpXHfpQp7ZuIsLrr2H5vXFv0JqsDCYSZKkfnPWURO4+YrT\n2dtygDf+173c/8zmvEsqaQYzSZLUr46dOprb/nIRDXVDedv1S/nBb9blXVLJ8iXmkiRpQGzZtY93\nfq2JZc9u4ayjJjC0yNGaVVXBJadOZ9HscX1c4cDo6iXmDpGQJEkDYuzIoXzjHafyz99/lOWrtxbd\nzqZd+/jhw+t4/9lzee+r5lI1pHLmSjOYSZKkAVNbU8W//clxvWpj194WPvTdR/j0HU/xwKrNfPot\nCxg/algfVZgv+5hJkqSyMnJYNZ+88Hj+/Y3H0bRqC6/77C+5p3lj3mX1CYOZJEkqOxHBhSdP43tX\nnkF9bTWXXL+UT9/xJK0HyrvvvMFMkiSVrXmT6lly5cv44xOm8Ok7nuJPv7KUDTv25l1W0RyVKUmS\nyl5KiW83reUfv/cII4ZWMb1hZNFtTRw1jL879yjmTBjVhxX+jqMyJUlSRTv4aPO4aaP57P89xa69\nrUW3tfSZzbzuM7/iPWfN4S/OnM3Q6oF7wOgdM0mSpDY27NjLR29/jO//+nnmTqjj4288lpNmHNZn\n7Xd1x8w+ZpIkSW2MHzWM/7x4AV+5bCG79rbwpi/cyz9+9xF27Nnf799tMJMkSerAq+ZN5H//6pVc\nevpMvrF0Ned88hf89LEX+/U77WMmSZLUibph1Xzk/PksPmEyV3/nYd75tSaOPrye2pr+ubdlMJMk\nSerGgulj+f57X8b1v3qGu/txMls7/0uSJA0gO/9LkiSVAYOZJElSiTCYSZIklQiDmSRJUokwmEmS\nJJUIg5kkSVKJMJhJkiSVCIOZJElSiTCYSZIklQiDmSRJUokwmEmSJJUIg5kkSVKJMJhJkiSViEgp\n5V1Dr0XEDmBF3nWUoXHAxryLKFNeu+J57YrjdSue1644XrfidXftZqSUxne0obp/6hlwK1JKC/Mu\notxERJPXrTheu+J57YrjdSue1644Xrfi9eba+ShTkiSpRBjMJEmSSkSlBLMv5V1AmfK6Fc9rVzyv\nXXG8bsXz2hXH61a8oq9dRXT+lyRJqgSVcsdMkiSp7JV1MIuIcyNiRUQ0R8RVeddTyiLiKxGxPiIe\nabPusIj4aUQ8lf0+Ns8aS1FETIuIn0XEYxHxaES8P1vvtetGRNRGxP0R8evs2v1ztn5WRCzNfm5v\njoiheddaiiKiKiIejIjbs89etx6IiFUR8XBEPBQRTdk6f157ICLGRMStEfFERDweEad77boWEUdl\nf9cO/toeEf+vN9etbINZRFQBnwfOAxqBiyOiMd+qStp/A+e2W3cV8H8ppbnA/2Wf9ftagL9OKTUC\npwHvyf6eee26txd4VUrpeOAE4NyIOA24BvhUSmkOsAW4PMcaS9n7gcfbfPa69dxZKaUT2kxX4M9r\nz3wG+HFKaR5wPIW/f167LqSUVmR/104ATgJ2A7fRi+tWtsEMOAVoTimtTCntA24CFudcU8lKKf0C\n2Nxu9WLghmz5BuCPB7SoMpBSWpdSWp4t76DwD9UUvHbdSgU7s4812a8EvAq4NVvvtetAREwFXg9c\nl30OvG694c9rNyJiNPAK4HqAlNK+lNJWvHaH4mzg6ZTSanpx3co5mE0B1rT5vDZbp56bmFJaly2/\nAEzMs5hSFxEzgQXAUrx2PZI9jnsIWA/8FHga2JpSasl28ee2Y58G/g44kH1uwOvWUwn434hYFhFX\nZOv8ee3eLGAD8NXsEfp1ETESr92huAi4MVsu+rqVczBTH0qF4bkO0e1ERNQB/wP8v5TS9rbbvHad\nSym1Zrf4p1K4yz0v55JKXkS8AVifUlqWdy1l6mUppRMpdHN5T0S8ou1Gf147VQ2cCPxXSmkBsIt2\nj9+8dp3L+nyeD3y7/bZDvW7lHMyeA6a1+Tw1W6eeezEiDgfIfl+fcz0lKSJqKISyb6aUvpOt9tod\nguyRyM+A04ExEXHwdXD+3P6hM4DzI2IVhS4ar6LQ98fr1gMppeey39dT6OtzCv689sRaYG1KaWn2\n+VYKQc1r1zPnActTSi9mn4u+buUczB4A5mYjlYZSuIW4JOeays0S4NJs+VLgeznWUpKyvj3XA4+n\nlD7ZZpPXrhsRMT4ixmTLw4FzKPTR+xnwpmw3r107KaWrU0pTU0ozKfy7dmdK6RK8bt2KiJERMerg\nMvAa4BH8ee1WSukFYE1EHJWtOht4DK9dT13M7x5jQi+uW1lPMBsRr6PQF6MK+EpK6WM5l1SyIuJG\n4EwKb7x/Efgw8F3gFmA6sBq4MKXUfoDAoBYRLwN+CTzM7/r7/AOFfmZeuy5ExHEUOr1WUfhP4C0p\npY9GxBEU7gQdBjwIvC2ltDe/SktXRJwJ/E1K6Q1et+5l1+i27GM18K2U0sciogF/XrsVESdQGHAy\nFFgJ/BnZzy5eu05l/wl4FjgipbQtW1f037myDmaSJEmVpJwfZUqSJFUUg5kkSVKJMJhJkiSVCIOZ\nJElSiTCYSZIklQiDmSRJUokwmEmSJJUIg5kkSVKJ+P8B73mbqeUMubsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pEjAnQK9Mhfy"
      },
      "source": [
        "#### **Let's evaluate the best model, on the validation set and compute relevant metrics:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NKttpsSSFyfD",
        "colab": {}
      },
      "source": [
        "#uploaded = files.upload()\n",
        "res_cnn.load_weights('base_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hkPgAlWuF0t4",
        "outputId": "420c6a31-037a-43f6-f0a3-0a689e1bff15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "X, y_true = next(val_generator)\n",
        "y_pred = res_cnn.predict(X)\n",
        "for i in range(1, len(val_generator)):\n",
        "  X, y = next(val_generator)\n",
        "  y_true = np.vstack((y_true, y))\n",
        "  y_pred = np.vstack((y_pred, res_cnn.predict(X)))\n",
        "\n",
        "y_true = np.argmax(y_true, axis=1)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "val_acc = accuracy_score(y_true, y_pred)\n",
        "#roc_auc = roc_auc_score(y_true, y_pred)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "class_names = [k for k in val_generator.class_indices]\n",
        "c_report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "\n",
        "print('\\nval_acc:\\n', val_acc)\n",
        "print('\\nConfusion Matrix:\\n', cm)\n",
        "print('\\nClassification Report:\\n', c_report)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_acc:\n",
            " 0.6\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 0  0 10]\n",
            " [ 0  9  1]\n",
            " [ 0  1  9]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           4       0.00      0.00      0.00        10\n",
            "           5       0.90      0.90      0.90        10\n",
            "   all_other       0.45      0.90      0.60        10\n",
            "\n",
            "    accuracy                           0.60        30\n",
            "   macro avg       0.45      0.60      0.50        30\n",
            "weighted avg       0.45      0.60      0.50        30\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}