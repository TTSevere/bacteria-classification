{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model C1 vs C2-3 vs all_other.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9725RbwsB_ja",
        "colab_type": "text"
      },
      "source": [
        "###  <span style=\"color:red\">**This Notebook can be run from Google Colab:**</span>\n",
        "\n",
        "https://colab.research.google.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks-1Im7ECAim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "import keras\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.layers import Input, Dense, Activation, Dropout, BatchNormalization,\\\n",
        "                          Conv2D, MaxPooling2D, Flatten, AveragePooling2D,\\\n",
        "                          GlobalAveragePooling2D, ZeroPadding2D\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import RMSprop, Adam, Adamax, Nadam, SGD\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, \\\n",
        "                            classification_report\n",
        "\n",
        "# Import PyDrive and associated libraries (to connect with GoogleDrive)\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# disable warnings\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DItexGaqdfEA",
        "colab_type": "text"
      },
      "source": [
        "### **Check if we are using GPU:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn7AYx74dNq6",
        "colab_type": "code",
        "outputId": "2a97e962-1577-42ba-875a-87124770b23a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import backend as K\n",
        "if K.backend() == \"tensorflow\":\n",
        "    import tensorflow as tf\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "    if device_name == '':\n",
        "        device_name = \"None\"\n",
        "    print('Using TensorFlow version:', tf.__version__, ', GPU:', device_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow version: 1.15.0 , GPU: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQXfhMrHlqrz",
        "colab_type": "text"
      },
      "source": [
        "### **Download Patches from GoogleDrive:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACvUlEucnnY0",
        "colab_type": "code",
        "outputId": "3700e672-7b83-48df-f7c4-851dbc8966b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = '1SM1aFESHTvMJa2bx3X6cd6HAwwYY3gPI' #NEW 128x128_s60_no border_minpospix_1250 minposval_1024 ROTATE_EVERY_45\n",
        "\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile(downloaded['title'])\n",
        "print('Downloaded content: \"{}\"'.format(downloaded['title']))\n",
        "print('Root dir content: {}'.format(os.listdir()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded content: \"Patches_new_128_s60_vs22_min1250_minval_1024_pos_only_no_border_nonNeg_rot_every_45_balanced_train-val.zip\"\n",
            "Root dir content: ['.config', 'Patches', 'gdrive', 'adc.json', 'base_model.h5', 'Patches_new_128_s60_vs22_min1250_minval_1024_pos_only_no_border_nonNeg_rot_every_45_balanced_train-val.zip', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSdGQav-qEJM",
        "colab_type": "text"
      },
      "source": [
        "### **Unzip the Patches:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP1-2THkn47w",
        "colab_type": "code",
        "outputId": "2de1f88b-78d3-4e95-f851-e7739448d4aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Remove 'Patches' dir if it already exists\n",
        "if 'Patches' in os.listdir():\n",
        "  shutil.rmtree('./Patches')\n",
        "with zipfile.ZipFile(downloaded['title'],\"r\") as zip:\n",
        "    zip.extractall()\n",
        "os.remove(downloaded['title'])\n",
        "print('Root dir content: {}'.format(os.listdir()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root dir content: ['.config', 'Patches', 'gdrive', 'adc.json', 'base_model.h5', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd7htfD3rJ6V",
        "colab_type": "text"
      },
      "source": [
        "### **Let's count patches by type and class:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjJy7FFsvm5H",
        "colab_type": "code",
        "outputId": "0b97f86a-038f-43a4-8cd7-9287f75cf693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "class_weights = {} # empty dictionary to store class weights\n",
        "classes = ['C1','C2-3','C4-7','C5','C6','C8','C9','C10']\n",
        "\n",
        "grand_total, pos_total, neg_total = 0, 0, 0\n",
        "for type in ['Serial', 'Control', 'Streak']:\n",
        "    print(\"\\nTotal '{}' Patches per location:\".format(type))\n",
        "    n_type, type_pos, type_neg = 0, 0, 0\n",
        "    class_weights[type] = {} # nested empty dictionary to store class weights\n",
        "    class_weights[type]['pos'] = {} # nested dictionary to store class weights\n",
        "    class_weights[type]['neg'] = {} # nested dictionary to store class weights\n",
        "    for cls in classes:\n",
        "        pos_folder = './Patches/Positive/{}/{}_pos'.format(type,cls)\n",
        "        neg_folder = './Patches/Negative/{}/{}_neg'.format(type,cls)\n",
        "        n_pos = len(os.listdir(pos_folder))\n",
        "        n_neg = len(os.listdir(neg_folder))\n",
        "        total = n_pos + n_neg\n",
        "        n_type += total\n",
        "        type_pos += n_pos\n",
        "        type_neg += n_neg\n",
        "        print('total_{}: {} = {} positive + {} negative'.format(cls,total,n_pos,n_neg))\n",
        "        class_weights[type]['pos']['{}'.format(cls)] = 1/n_pos if n_pos else 0\n",
        "        class_weights[type]['neg']['{}'.format(cls)] = 1/n_neg if n_neg else 0\n",
        "    print('Total {}: {} = {} positive + {} negative'.format(type,n_type,type_pos,type_neg))\n",
        "    for loc in class_weights[type]['pos'].keys():\n",
        "        class_weights[type]['pos'][loc] *= type_pos\n",
        "    for loc in class_weights[type]['neg'].keys():\n",
        "        class_weights[type]['neg'][loc] *= type_neg\n",
        "    grand_total += n_type\n",
        "    pos_total += type_pos\n",
        "    neg_total += type_neg\n",
        "print('\\nGRAND TOTAL: {} = {} positive + {} negative'.format(grand_total,pos_total,neg_total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total 'Serial' Patches per location:\n",
            "total_C1: 5998 = 5998 positive + 0 negative\n",
            "total_C2-3: 5998 = 5998 positive + 0 negative\n",
            "total_C4-7: 5998 = 5998 positive + 0 negative\n",
            "total_C5: 5998 = 5998 positive + 0 negative\n",
            "total_C6: 5998 = 5998 positive + 0 negative\n",
            "total_C8: 5998 = 5998 positive + 0 negative\n",
            "total_C9: 4100 = 4100 positive + 0 negative\n",
            "total_C10: 5998 = 5998 positive + 0 negative\n",
            "Total Serial: 46086 = 46086 positive + 0 negative\n",
            "\n",
            "Total 'Control' Patches per location:\n",
            "total_C1: 364 = 364 positive + 0 negative\n",
            "total_C2-3: 364 = 364 positive + 0 negative\n",
            "total_C4-7: 364 = 364 positive + 0 negative\n",
            "total_C5: 364 = 364 positive + 0 negative\n",
            "total_C6: 364 = 364 positive + 0 negative\n",
            "total_C8: 364 = 364 positive + 0 negative\n",
            "total_C9: 364 = 364 positive + 0 negative\n",
            "total_C10: 364 = 364 positive + 0 negative\n",
            "Total Control: 2912 = 2912 positive + 0 negative\n",
            "\n",
            "Total 'Streak' Patches per location:\n",
            "total_C1: 0 = 0 positive + 0 negative\n",
            "total_C2-3: 0 = 0 positive + 0 negative\n",
            "total_C4-7: 0 = 0 positive + 0 negative\n",
            "total_C5: 0 = 0 positive + 0 negative\n",
            "total_C6: 0 = 0 positive + 0 negative\n",
            "total_C8: 0 = 0 positive + 0 negative\n",
            "total_C9: 0 = 0 positive + 0 negative\n",
            "total_C10: 0 = 0 positive + 0 negative\n",
            "Total Streak: 0 = 0 positive + 0 negative\n",
            "\n",
            "GRAND TOTAL: 48998 = 48998 positive + 0 negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLfSmpKFPLFu",
        "colab_type": "text"
      },
      "source": [
        "### **Let's downsample training ('Serial') classes to no more than the minimum between 'C1' and 'C2-3':**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPt4zNrWPJed",
        "colab_type": "code",
        "outputId": "e8278397-1205-4f2f-ab50-c8aa03515265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "train_type = 'Serial'\n",
        "print('Before downsampling training patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pos_folder = './Patches/Positive/{}/{}_pos'.format(train_type,cls)\n",
        "    neg_folder = './Patches/Negative/{}/{}_neg'.format(train_type,cls)\n",
        "    n_pos = len(os.listdir(pos_folder))\n",
        "    n_neg = len(os.listdir(neg_folder))\n",
        "    total = n_pos + n_neg\n",
        "    n_type += total\n",
        "    type_pos += n_pos\n",
        "    type_neg += n_neg\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,n_pos,n_neg))\n",
        "    totals['{}'.format(cls)] = n_pos if n_pos else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(train_type,n_type,type_pos,type_neg))\n",
        "\n",
        "if totals['C1'] < totals['C2-3']:\n",
        "    minority = 'C1'\n",
        "else:\n",
        "    minority = 'C2-3'\n",
        "n_min = totals[minority]\n",
        "for key, value in totals.items():\n",
        "    #if key != minority and key not in ['C1','C2-3']:\n",
        "    if key != minority:\n",
        "        n_to_delete = max(value-n_min, 0)\n",
        "        root = './Patches/Positive/{}/{}_pos/'.format(train_type,key)\n",
        "        patches = os.listdir(root)\n",
        "        patches_to_delete = np.random.choice(patches, n_to_delete, replace=False)\n",
        "        for patch in patches_to_delete:\n",
        "            pass\n",
        "            os.remove(root + patch)\n",
        "\n",
        "print('After downsampling validation patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pos_folder = './Patches/Positive/{}/{}_pos'.format(train_type,cls)\n",
        "    neg_folder = './Patches/Negative/{}/{}_neg'.format(train_type,cls)\n",
        "    n_pos = len(os.listdir(pos_folder))\n",
        "    n_neg = len(os.listdir(neg_folder))\n",
        "    total = n_pos + n_neg\n",
        "    n_type += total\n",
        "    type_pos += n_pos\n",
        "    type_neg += n_neg\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,n_pos,n_neg))\n",
        "    totals['{}'.format(cls)] = n_pos if n_pos else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(train_type,n_type,type_pos,type_neg))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before downsampling training patches:\n",
            "total_C1: 5998 = 5998 positive + 0 negative\n",
            "total_C2-3: 5998 = 5998 positive + 0 negative\n",
            "total_C4-7: 5998 = 5998 positive + 0 negative\n",
            "total_C5: 5998 = 5998 positive + 0 negative\n",
            "total_C6: 5998 = 5998 positive + 0 negative\n",
            "total_C8: 5998 = 5998 positive + 0 negative\n",
            "total_C9: 4100 = 4100 positive + 0 negative\n",
            "total_C10: 5998 = 5998 positive + 0 negative\n",
            "Total Serial: 46086 = 46086 positive + 0 negative\n",
            "\n",
            "After downsampling validation patches:\n",
            "total_C1: 5998 = 5998 positive + 0 negative\n",
            "total_C2-3: 5998 = 5998 positive + 0 negative\n",
            "total_C4-7: 5998 = 5998 positive + 0 negative\n",
            "total_C5: 5998 = 5998 positive + 0 negative\n",
            "total_C6: 5998 = 5998 positive + 0 negative\n",
            "total_C8: 5998 = 5998 positive + 0 negative\n",
            "total_C9: 4100 = 4100 positive + 0 negative\n",
            "total_C10: 5998 = 5998 positive + 0 negative\n",
            "Total Serial: 46086 = 46086 positive + 0 negative\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8a8ZcfKKvw5",
        "colab_type": "text"
      },
      "source": [
        "### **Let's move all patches for classes other than C1 or C2-3, to a single folder called 'all_other':**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz9V2uuxLM50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for patch_type in ['Positive','Negative']:\n",
        "    if patch_type == 'Positive': sufix = '_pos'\n",
        "    if patch_type == 'Negative': sufix = '_neg'\n",
        "    for type_ in ['Serial', 'Control', 'Streak']:\n",
        "        folder = './Patches/{}/{}/'.format(patch_type,type_)\n",
        "        if 'all_other' + sufix not in os.listdir(folder):\n",
        "            dest = folder + 'all_other' + sufix\n",
        "            os.mkdir(dest)\n",
        "\n",
        "        for cls in classes:\n",
        "            if cls in ['C1','C2-3']: continue\n",
        "            cls_folder = folder + cls + sufix\n",
        "            for patch in os.listdir(cls_folder):\n",
        "                source = cls_folder + '/' + patch\n",
        "                shutil.move(source, dest)\n",
        "            shutil.rmtree(cls_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sPAFxK7REM7",
        "colab_type": "text"
      },
      "source": [
        "### **Let's count patches by type and class again:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmKlpYPZRE-_",
        "colab_type": "code",
        "outputId": "9cd4a9d2-12f9-4727-852c-c94ff6768310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "class_weights = {} # empty dictionary to store class weights\n",
        "classes = ['C1','C2-3','all_other']\n",
        "grand_total, pos_total, neg_total = 0, 0, 0\n",
        "\n",
        "for type in ['Serial', 'Control', 'Streak']:\n",
        "    print(\"\\nTotal '{}' Patches per location:\".format(type))\n",
        "    n_type, type_pos, type_neg = 0, 0, 0\n",
        "    class_weights[type] = {} # nested empty dictionary to store class weights\n",
        "    class_weights[type]['pos'] = {} # nested dictionary to store class weights\n",
        "    class_weights[type]['neg'] = {} # nested dictionary to store class weights\n",
        "    for cls in classes:\n",
        "        pos_folder = './Patches/Positive/{}/{}_pos'.format(type,cls)\n",
        "        neg_folder = './Patches/Negative/{}/{}_neg'.format(type,cls)\n",
        "        n_pos = len(os.listdir(pos_folder))\n",
        "        n_neg = len(os.listdir(neg_folder))\n",
        "        total = n_pos + n_neg\n",
        "        n_type += total\n",
        "        type_pos += n_pos\n",
        "        type_neg += n_neg\n",
        "        print('total_{}: {} = {} positive + {} negative'.format(cls,total,n_pos,n_neg))\n",
        "        class_weights[type]['pos']['{}'.format(cls)] = 1/n_pos if n_pos else 0\n",
        "        class_weights[type]['neg']['{}'.format(cls)] = 1/n_neg if n_neg else 0\n",
        "    print('Total {}: {} = {} positive + {} negative'.format(type,n_type,type_pos,type_neg))\n",
        "    for loc in class_weights[type]['pos'].keys():\n",
        "        class_weights[type]['pos'][loc] *= type_pos\n",
        "    for loc in class_weights[type]['neg'].keys():\n",
        "        class_weights[type]['neg'][loc] *= type_neg\n",
        "    grand_total += n_type\n",
        "    pos_total += type_pos\n",
        "    neg_total += type_neg\n",
        "print('\\nGRAND TOTAL: {} = {} positive + {} negative'.format(grand_total,pos_total,neg_total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total 'Serial' Patches per location:\n",
            "total_C1: 5998 = 5998 positive + 0 negative\n",
            "total_C2-3: 5998 = 5998 positive + 0 negative\n",
            "total_all_other: 34090 = 34090 positive + 0 negative\n",
            "Total Serial: 46086 = 46086 positive + 0 negative\n",
            "\n",
            "Total 'Control' Patches per location:\n",
            "total_C1: 364 = 364 positive + 0 negative\n",
            "total_C2-3: 364 = 364 positive + 0 negative\n",
            "total_all_other: 2184 = 2184 positive + 0 negative\n",
            "Total Control: 2912 = 2912 positive + 0 negative\n",
            "\n",
            "Total 'Streak' Patches per location:\n",
            "total_C1: 0 = 0 positive + 0 negative\n",
            "total_C2-3: 0 = 0 positive + 0 negative\n",
            "total_all_other: 0 = 0 positive + 0 negative\n",
            "Total Streak: 0 = 0 positive + 0 negative\n",
            "\n",
            "GRAND TOTAL: 48998 = 48998 positive + 0 negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb9BMr39R9LH",
        "colab_type": "text"
      },
      "source": [
        "### **Since we want to focus on 'C1' and 'C2-3', let's downsample again:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31ZsitULR-Ap",
        "colab_type": "code",
        "outputId": "be9d5dd2-982a-4bbe-fc40-d6a0a7603b34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "train_type = 'Serial'\n",
        "print('Before downsampling training patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pos_folder = './Patches/Positive/{}/{}_pos'.format(train_type,cls)\n",
        "    neg_folder = './Patches/Negative/{}/{}_neg'.format(train_type,cls)\n",
        "    n_pos = len(os.listdir(pos_folder))\n",
        "    n_neg = len(os.listdir(neg_folder))\n",
        "    total = n_pos + n_neg\n",
        "    n_type += total\n",
        "    type_pos += n_pos\n",
        "    type_neg += n_neg\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,n_pos,n_neg))\n",
        "    totals['{}'.format(cls)] = n_pos if n_pos else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(train_type,n_type,type_pos,type_neg))\n",
        "\n",
        "minority = min(totals, key=totals.get)\n",
        "n_min = totals[minority]\n",
        "for key, value in totals.items():\n",
        "    if key != minority and key not in ['C1','C2-3']:\n",
        "        n_to_delete = value - n_min\n",
        "        root = './Patches/Positive/{}/{}_pos/'.format(train_type,key)\n",
        "        patches = os.listdir(root)\n",
        "        patches_to_delete = np.random.choice(patches, n_to_delete, replace=False)\n",
        "        for patch in patches_to_delete:\n",
        "            pass\n",
        "            os.remove(root + patch)\n",
        "\n",
        "print('After downsampling validation patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pos_folder = './Patches/Positive/{}/{}_pos'.format(train_type,cls)\n",
        "    neg_folder = './Patches/Negative/{}/{}_neg'.format(train_type,cls)\n",
        "    n_pos = len(os.listdir(pos_folder))\n",
        "    n_neg = len(os.listdir(neg_folder))\n",
        "    total = n_pos + n_neg\n",
        "    n_type += total\n",
        "    type_pos += n_pos\n",
        "    type_neg += n_neg\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,n_pos,n_neg))\n",
        "    class_weights[train_type]['pos']['{}'.format(cls)] = 1/n_pos if n_pos else 0\n",
        "    totals['{}'.format(cls)] = n_pos if n_pos else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(train_type,n_type,type_pos,type_neg))\n",
        "for loc in class_weights[train_type]['pos'].keys():\n",
        "    class_weights[train_type]['pos'][loc] *= type_pos"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before downsampling training patches:\n",
            "total_C1: 5998 = 5998 positive + 0 negative\n",
            "total_C2-3: 5998 = 5998 positive + 0 negative\n",
            "total_all_other: 34090 = 34090 positive + 0 negative\n",
            "Total Serial: 46086 = 46086 positive + 0 negative\n",
            "\n",
            "After downsampling validation patches:\n",
            "total_C1: 5998 = 5998 positive + 0 negative\n",
            "total_C2-3: 5998 = 5998 positive + 0 negative\n",
            "total_all_other: 5998 = 5998 positive + 0 negative\n",
            "Total Serial: 17994 = 17994 positive + 0 negative\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pnI1G0ywLOp",
        "colab_type": "text"
      },
      "source": [
        "#### **Since we still have slightly imbalanced training data, we have set different class weights to give more importance to the minority classes:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCcbwQX7mKQq",
        "colab_type": "code",
        "outputId": "c50288da-5be1-47bd-f625-021a619b3a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "print('Class Weights:', str(json.dumps(class_weights['Serial'], indent=2, default=str)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class Weights: {\n",
            "  \"pos\": {\n",
            "    \"C1\": 3.0,\n",
            "    \"C2-3\": 3.0,\n",
            "    \"all_other\": 3.0\n",
            "  },\n",
            "  \"neg\": {\n",
            "    \"C1\": 0,\n",
            "    \"C2-3\": 0,\n",
            "    \"all_other\": 0\n",
            "  }\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2gctgbsF-fF",
        "colab_type": "text"
      },
      "source": [
        "### **Let's downsample positive majority classes in validation ('Control')patches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJETu-bSF_R8",
        "colab_type": "code",
        "outputId": "ad0dc600-33b7-44a9-ebe6-eb2765959261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "val_type = 'Control'\n",
        "print('Before downsampling validation patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pos_folder = './Patches/Positive/{}/{}_pos'.format(val_type,cls)\n",
        "    neg_folder = './Patches/Negative/{}/{}_neg'.format(val_type,cls)\n",
        "    n_pos = len(os.listdir(pos_folder))\n",
        "    n_neg = len(os.listdir(neg_folder))\n",
        "    total = n_pos + n_neg\n",
        "    n_type += total\n",
        "    type_pos += n_pos\n",
        "    type_neg += n_neg\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,n_pos,n_neg))\n",
        "    totals['{}'.format(cls)] = n_pos if n_pos else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(val_type,n_type,type_pos,type_neg))\n",
        "\n",
        "minority = min(totals, key=totals.get)\n",
        "n_min = totals[minority]\n",
        "for key, value in totals.items():\n",
        "    if key != minority:\n",
        "        n_to_delete = value - n_min\n",
        "        root = './Patches/Positive/{}/{}_pos/'.format(val_type,key)\n",
        "        patches = os.listdir(root)\n",
        "        patches_to_delete = np.random.choice(patches, n_to_delete, replace=False)\n",
        "        for patch in patches_to_delete:\n",
        "            os.remove(root + patch)\n",
        "\n",
        "print('After downsampling validation patches:')\n",
        "totals = {}\n",
        "n_type, type_pos, type_neg = 0, 0, 0\n",
        "for cls in classes:\n",
        "    pos_folder = './Patches/Positive/{}/{}_pos'.format(val_type,cls)\n",
        "    neg_folder = './Patches/Negative/{}/{}_neg'.format(val_type,cls)\n",
        "    n_pos = len(os.listdir(pos_folder))\n",
        "    n_neg = len(os.listdir(neg_folder))\n",
        "    total = n_pos + n_neg\n",
        "    n_type += total\n",
        "    type_pos += n_pos\n",
        "    type_neg += n_neg\n",
        "    print('total_{}: {} = {} positive + {} negative'.format(cls,total,n_pos,n_neg))\n",
        "    totals['{}'.format(cls)] = n_pos if n_pos else 0\n",
        "print('Total {}: {} = {} positive + {} negative\\n'.format(val_type,n_type,type_pos,type_neg))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before downsampling validation patches:\n",
            "total_C1: 364 = 364 positive + 0 negative\n",
            "total_C2-3: 364 = 364 positive + 0 negative\n",
            "total_all_other: 2184 = 2184 positive + 0 negative\n",
            "Total Control: 2912 = 2912 positive + 0 negative\n",
            "\n",
            "After downsampling validation patches:\n",
            "total_C1: 364 = 364 positive + 0 negative\n",
            "total_C2-3: 364 = 364 positive + 0 negative\n",
            "total_all_other: 364 = 364 positive + 0 negative\n",
            "Total Control: 1092 = 1092 positive + 0 negative\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNJ7lSKQpdOT",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's build image generators, using keras.preprocessing.image.ImageDataGenerator, rescaling image pixel values from [0,  255] to [0, 1]:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UANfdA6IUFKt",
        "colab_type": "code",
        "outputId": "19c15f56-dd84-4132-aa08-7cde5965c6fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "c1_pos_folder = './Patches/Positive/Serial/C1_pos'\n",
        "img = plt.imread(c1_pos_folder + '/' + os.listdir(c1_pos_folder)[:5][0])\n",
        "img_size = img.shape\n",
        "train_batch_size = 24\n",
        "val_batch_size = 64\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "print(\"For training:\")\n",
        "train_generator = datagen.flow_from_directory(\n",
        "        './Patches/Positive/Serial',\n",
        "        target_size=(img_size[0],img_size[1]),\n",
        "        batch_size=train_batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True)\n",
        "\n",
        "print(\"\\nFor validation:\")\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "        './Patches/Positive/Control',\n",
        "        target_size=(img_size[0],img_size[1]),\n",
        "        batch_size=val_batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For training:\n",
            "Found 17994 images belonging to 3 classes.\n",
            "\n",
            "For validation:\n",
            "Found 1092 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ZzUGXvwqUn",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's check what is the training generator's index for each class, so we can correclty set up the class weights:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSHQNC2awn84",
        "colab_type": "code",
        "outputId": "da8e37c5-76b5-4093-c7a4-4da0aa09e664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('train_generator.class_indices:', str(json.dumps(train_generator.class_indices, indent=2, default=str)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_generator.class_indices: {\n",
            "  \"C1_pos\": 0,\n",
            "  \"C2-3_pos\": 1,\n",
            "  \"all_other_pos\": 2\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o30MT4e2O7gk",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's set up the class weights in correct order:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2gsR9n6w2vm",
        "colab_type": "code",
        "outputId": "1eaa0d5e-6849-4098-9461-e09c272723d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "serial_pos_weights = []\n",
        "for cls in classes:\n",
        "    serial_pos_weights.append(class_weights['Serial']['pos']['{}'.format(cls)])\n",
        "print('original class weights dictionary:')\n",
        "print(str(json.dumps(class_weights['Serial']['pos'], indent=2, default=str)))\n",
        "print('class weights for generator, re-arranging indexes:')\n",
        "print(str(json.dumps(serial_pos_weights, indent=2, default=str)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original class weights dictionary:\n",
            "{\n",
            "  \"C1\": 3.0,\n",
            "  \"C2-3\": 3.0,\n",
            "  \"all_other\": 3.0\n",
            "}\n",
            "class weights for generator, re-arranging indexes:\n",
            "[\n",
            "  3.0,\n",
            "  3.0,\n",
            "  3.0\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdXBPBsVxA39",
        "colab_type": "text"
      },
      "source": [
        "## **Let's build our Base Model. We will use Resnet:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjNmNKdHw4tM",
        "colab_type": "text"
      },
      "source": [
        "#### **First. let's create a function to build a residual block.**\n",
        "\n",
        "#### We will use the residual block proposed in  [ResNetV2](https://arxiv.org/pdf/1603.05027.pdf) and will implement it by ourselves:\n",
        "\n",
        "\n",
        ">![Google's logo](https://camo.githubusercontent.com/7ae470c333cd76078e1c669055ad98bcedaf523f/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3130303532332f61313536613563322d303236622d646535352d613666622d6534666131373732623432632e706e67)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-DqZH20w3ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def res_block(X, filters, kernel_size=(3,3), l2_reg=1e-6, residual=True,\n",
        "              first=False, subsampling=False):\n",
        "    \"\"\"\n",
        "    Function to build a residual block as proposed in ResNetV2:\n",
        "             https://arxiv.org/pdf/1603.05027.pdf:\n",
        "    :param X: The input to the residual block\n",
        "    :param filters: Integer. Number of filters / channels in the output\n",
        "    :param kernel_size: Tuple (Int, Int). kernel size for convolution operations\n",
        "    :param l2_reg: Float. L2 norm for L2 regularization\n",
        "    :param residual: Boolean. True if residual block. Otherwise 'plain' block.\n",
        "    :param first: Boolean. True if first residual block -> ZeroPad and Maxpool.\n",
        "    :param subsampling: Boolean. True if subsampling within the residual block.\n",
        "    :return: the addition output from the residual block proposed in ResNetV2:\n",
        "              https://arxiv.org/pdf/1603.05027.pdf\n",
        "    \"\"\"\n",
        "    bn = BatchNormalization()(X)\n",
        "    relu = Activation(\"relu\")(bn)\n",
        "    \n",
        "    if first: #The first layer is subsampled with Maxpool\n",
        "      pad = ZeroPadding2D(padding=(1, 1))(relu)\n",
        "      relu = MaxPooling2D(pool_size=(3, 3), strides=(2,2))(pad)    \n",
        "    \n",
        "    if subsampling: #Resnet reduces size just by using stride=2 instead of pool\n",
        "      #Here we will reduce the size (subsample) by using stride 2 \n",
        "      conv_1 = Conv2D(filters, kernel_size, strides=(2,2), padding='same',\n",
        "                         kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                         kernel_initializer = glorot_uniform(0),\n",
        "                         bias_initializer = glorot_uniform(0))(relu)\n",
        "      if residual:\n",
        "        #To be able to add, we also need to reduce size of input\n",
        "        #For this, we will just use a 1x1 Conv2D with stride 2  \n",
        "        res = Conv2D(filters, kernel_size=[1,1], strides=(2,2),\n",
        "                     padding='same')(X)\n",
        "    else: #No subsampling, same size as input\n",
        "      conv_1 = Conv2D(filters, kernel_size, strides=(1,1), padding='same',\n",
        "                   kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                   kernel_initializer = glorot_uniform(0),\n",
        "                   bias_initializer = glorot_uniform(0))(relu)\n",
        "      if residual:\n",
        "        if first: #The first layer is subsampled with Maxpool so, resize X\n",
        "          #For this, we will just use a 1x1 Conv2D with stride 2\n",
        "          res = Conv2D(filters, kernel_size=[1,1], strides=(2,2),\n",
        "                     padding='same')(X)\n",
        "        else:\n",
        "          res = X\n",
        "    bn = BatchNormalization()(conv_1)\n",
        "    relu = Activation(\"relu\")(bn)\n",
        "\n",
        "    conv_2 = Conv2D(filters, kernel_size, padding='same',\n",
        "                       kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                       kernel_initializer = glorot_uniform(0),\n",
        "                       bias_initializer = glorot_uniform(0))(relu)\n",
        "    if residual:\n",
        "      add = keras.layers.add([res, conv_2])\n",
        "      return add\n",
        "    else:\n",
        "      return conv_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K3-02NNUXOV",
        "colab_type": "text"
      },
      "source": [
        "#### **Second, let's create a function to build a Resnet network:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32s7rai-609p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from keras.layers import AveragePooling2D, GlobalAveragePooling2D\n",
        "\n",
        "def make_resnet(img_size, n_classes, layers_per_group, n_filters, \n",
        "                    kernel_sizes, l2_reg=1e-6, optimizer=keras.optimizers.SGD,\n",
        "                    lr=1e-1, decay=1e-4, momentum=0.9, residual=True):\n",
        "  \n",
        "    \"\"\"\n",
        "    Function to build ResNet network, but with some user defined parameters.\n",
        "        \n",
        "        ResNet network input size is (224,224,3) then it starts with one \n",
        "        convolution layer, (7x7x64, stride 2) followed by maxpool (3x3, stride2) \n",
        "        then it will build 4 layer groups and the user will define the number of\n",
        "        residual layers per group.\n",
        "        \n",
        "        As example, ResNet34, after the first convolution layer, it has 4 groups\n",
        "        of layers with the following number of residual 'layers_per_group':\n",
        "        [6,8,12,6]\n",
        "        Because the residual connections are made between pair of layers, the\n",
        "        number of layers for each group must be a pair number.\n",
        "        \n",
        "        The user will also be able to define the number of filters and size of\n",
        "        each filter, independently for each group of layers. All layers in the\n",
        "        same group group will have the same number of filters and each filter\n",
        "        within the group will have the same size.\n",
        "        \n",
        "        Same as ResNet, an average pooling layer follows after the 4 groups\n",
        "        of layers.\n",
        "    \n",
        "    :param img_size: Size of input image, in the form: (size, size, #channels)\n",
        "    :param n_classes: Integer. Number of classes for classification.\n",
        "    :param layers_per_group: List with # of layers per group (e.g.[6,8,12,6])\n",
        "    :param n_filters: List of length 4, with number of filters for each group of\n",
        "                      layers.\n",
        "    :param kernel_sizes: List of length 4, with kernel sizes tuples for each \n",
        "                          group of layers\n",
        "    :param l2_reg: Float. L2 norm for L2 regularization\n",
        "    :param optimizer: A keras optimizer from keras.optimizers\n",
        "    :param lr: Float. learning rate for the optimizer.\n",
        "    :param decay: Float. learning rate decay for the optimizer.\n",
        "    :param momentum: Float. Momentum for SGD if optimizer=keras.optimizers.SGD\n",
        "    :param residual: Boolean. True if residual net. Otherwise 'plain' net.\n",
        "    :return: CNN with residual connections, similar to ResNet32\n",
        "    \"\"\"\n",
        "    \n",
        "    if len(layers_per_block) != len(n_filters) or len(layers_per_block) !=\\\n",
        "        len(kernel_sizes) or len(n_filters) != len(n_filters):\n",
        "        e = \"Length of 'layers_per_block', 'n_filters' and 'kernel_sizes'\" +\\\n",
        "        \" must be the same\"\n",
        "        raise Exception(e)  \n",
        "\n",
        "    for layers in layers_per_block:\n",
        "      if layers % 2 == 1:\n",
        "        e = \"Number of 'layers_per_block' must be even/pair numbers\"\n",
        "        raise Exception(e)\n",
        "      \n",
        "    inputs = Input(shape=img_size)\n",
        "    \n",
        "    n_filters_conv1 = 64\n",
        "    kernel_sizes_conv1 = (3,3) # kernel size for very first conv layer\n",
        "    strides_conv1 = (2,2)\n",
        "    \n",
        "    conv1 = Conv2D(n_filters_conv1, kernel_sizes_conv1, strides=strides_conv1,\n",
        "                   padding='same', kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                   kernel_initializer = glorot_uniform(0),\n",
        "                   bias_initializer = glorot_uniform(0))(inputs)\n",
        "    \n",
        "    layer_count = 1 # counter for the number of layers\n",
        "    for i in range(len(layers_per_block)):\n",
        "      for j in range(int(layers_per_block[i]/2)):\n",
        "        if j == 0:\n",
        "          if i == 0:\n",
        "            add = res_block(conv1, n_filters[i], kernel_sizes[i], l2_reg,\n",
        "                            residual, first=True)\n",
        "            #add = res_block(conv0_1, n_filters[i], kernel_sizes[i], l2_reg,\n",
        "            #                residual, subsampling=True)\n",
        "          else:\n",
        "            add = res_block(add, n_filters[i], kernel_sizes[i], l2_reg,\n",
        "                            residual, subsampling=True)\n",
        "        else:\n",
        "          add = res_block(add, n_filters[i], kernel_sizes[i], l2_reg,\n",
        "                          residual)\n",
        "          \n",
        "    bn = BatchNormalization()(add)\n",
        "    relu = Activation(\"relu\")(bn)\n",
        "\n",
        "    flat = GlobalAveragePooling2D()(relu)\n",
        "\n",
        "    out = Dense(n_classes, activation='softmax',\n",
        "                      kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                      kernel_initializer=glorot_uniform(0),\n",
        "                      bias_initializer=glorot_uniform(0))(flat)\n",
        "\n",
        "    res_cnn = Model(inputs=inputs, outputs=out)\n",
        "\n",
        "    if optimizer == keras.optimizers.Nadam:\n",
        "        res_cnn.compile(optimizer(lr=lr, schedule_decay=decay),\n",
        "                    \"categorical_crossentropy\", metrics=['accuracy'])\n",
        "    elif optimizer == keras.optimizers.SGD:\n",
        "        res_cnn.compile(optimizer(lr=lr, momentum=momentum, decay=decay),\n",
        "                        \"categorical_crossentropy\", metrics=['accuracy'])\n",
        "    else:\n",
        "        res_cnn.compile(optimizer(lr=lr, decay=decay),\n",
        "                        \"categorical_crossentropy\", metrics=['accuracy'])\n",
        "    return res_cnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m21vIzndbOIZ",
        "colab_type": "text"
      },
      "source": [
        "### **Let's build a ResNet18:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLvSCOTsAuKF",
        "colab_type": "code",
        "outputId": "8bc32ce3-e978-4a81-d7c6-ae617daa50dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "classes = list(iter(train_generator.class_indices))\n",
        "n_classes = len(classes)\n",
        "layers_per_block = [4, 4, 4, 4] #18 layers total with first conv and last FC\n",
        "n_filters = [64, 128, 256, 512]\n",
        "kernel_sizes = [(3,3), (3,3), (3,3), (3,3)]\n",
        "l2_reg = 0.1\n",
        "optimizer = RMSprop # Adamax, RMSprop, Adam (No: Nadam, SGD)\n",
        "lr = 1e-3\n",
        "decay = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "res_cnn = make_resnet(img_size, n_classes, layers_per_block, n_filters,\n",
        "                       kernel_sizes, l2_reg, optimizer, lr, decay, momentum)\n",
        "res_cnn.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 64, 64, 64)   1792        input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 64, 64, 64)   256         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 64, 64, 64)   0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_4 (ZeroPadding2D (None, 66, 66, 64)   0           activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 64)   0           zero_padding2d_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 32, 32, 64)   36928       max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 32, 32, 64)   256         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 32, 32, 64)   0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 32, 32, 64)   4160        conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 32, 32, 64)   36928       activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 32, 32, 64)   0           conv2d_66[0][0]                  \n",
            "                                                                 conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 32, 32, 64)   256         add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 32, 32, 64)   0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 32, 32, 64)   36928       activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 32, 32, 64)   256         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 32, 32, 64)   0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 32, 32, 64)   36928       activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 32, 32, 64)   0           add_25[0][0]                     \n",
            "                                                                 conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 32, 32, 64)   256         add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 32, 32, 64)   0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 16, 16, 128)  73856       activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 16, 16, 128)  512         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 16, 16, 128)  0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 16, 16, 128)  8320        add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 16, 16, 128)  147584      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 16, 16, 128)  0           conv2d_71[0][0]                  \n",
            "                                                                 conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 16, 16, 128)  512         add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 16, 16, 128)  0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 16, 16, 128)  147584      activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 16, 16, 128)  512         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 16, 16, 128)  0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 16, 16, 128)  147584      activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_28 (Add)                    (None, 16, 16, 128)  0           add_27[0][0]                     \n",
            "                                                                 conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 16, 16, 128)  512         add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 16, 16, 128)  0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 8, 8, 256)    295168      activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 8, 8, 256)    1024        conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 8, 8, 256)    0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 8, 8, 256)    33024       add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 8, 8, 256)    590080      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_29 (Add)                    (None, 8, 8, 256)    0           conv2d_76[0][0]                  \n",
            "                                                                 conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 8, 8, 256)    1024        add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 8, 8, 256)    0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 8, 8, 256)    590080      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 8, 8, 256)    1024        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 8, 8, 256)    0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 8, 8, 256)    590080      activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_30 (Add)                    (None, 8, 8, 256)    0           add_29[0][0]                     \n",
            "                                                                 conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 8, 8, 256)    1024        add_30[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 8, 8, 256)    0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 4, 4, 512)    1180160     activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 4, 4, 512)    2048        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 4, 4, 512)    0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 4, 4, 512)    131584      add_30[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 4, 4, 512)    2359808     activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_31 (Add)                    (None, 4, 4, 512)    0           conv2d_81[0][0]                  \n",
            "                                                                 conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 4, 4, 512)    2048        add_31[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 4, 4, 512)    0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 4, 4, 512)    2359808     activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 4, 4, 512)    2048        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 4, 4, 512)    0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 4, 4, 512)    2359808     activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_32 (Add)                    (None, 4, 4, 512)    0           add_31[0][0]                     \n",
            "                                                                 conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 4, 4, 512)    2048        add_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 4, 4, 512)    0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_4 (Glo (None, 512)          0           activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 3)            1539        global_average_pooling2d_4[0][0] \n",
            "==================================================================================================\n",
            "Total params: 11,185,347\n",
            "Trainable params: 11,177,539\n",
            "Non-trainable params: 7,808\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Snp4w6_wHP_",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's mount our GoogleDrive to download the best model (We have to authenticate again):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7NpUL6XwImK",
        "colab_type": "code",
        "outputId": "c9e20251-fb0d-47ed-fd98-d0f520cd88de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUL3HGj4dyft",
        "colab_type": "text"
      },
      "source": [
        "### **Let's train and validate our Base Model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDWJ135lVCbR",
        "colab_type": "code",
        "outputId": "1fe874f2-259a-411d-b4c7-2bb8bc99d19a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## fully balanced training\n",
        "## Rotate every 45\n",
        "## 'C1' vs 'C2-3' vs 'all_other' - Downsampling training\n",
        "\n",
        "## 128x128, stride_60,\n",
        "##min_pos_pix_1250, mivalpos_1024\n",
        "## ReduceLROnPlateau(monitor='val_loss'... )\n",
        "## train_batch_32, opt_RMSprop, Kernel_3x3:\n",
        "\n",
        "epochs = 70\n",
        "\n",
        "train_steps = train_generator.n//train_generator.batch_size\n",
        "val_steps = val_generator.n//val_generator.batch_size\n",
        "\n",
        "# Callbacks:\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.85, patience=3, \n",
        "                                   verbose=1, mode='min', min_lr=1e-9)\n",
        "EarlyStop = EarlyStopping(monitor='val_acc', patience=70, verbose=1,\n",
        "                          min_delta=0, mode='max')\n",
        "checkpoint = ModelCheckpoint('base_model.h5', monitor='val_acc', verbose=1, \n",
        "                             save_best_only=True, mode='max')\n",
        "\n",
        "callbacks_list = [reduce_lr, checkpoint, EarlyStop] #order matters!\n",
        "\n",
        "#res_cnn.load_weights('base_model.h5')\n",
        "\n",
        "history = res_cnn.fit_generator(train_generator, steps_per_epoch=train_steps,\n",
        "                            validation_data=val_generator,\n",
        "                            validation_steps=val_steps, epochs=epochs,\n",
        "                            verbose=1, callbacks=callbacks_list, shuffle=False,\n",
        "                            class_weight=serial_pos_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "749/749 [==============================] - 33s 44ms/step - loss: 5.2209 - acc: 0.5755 - val_loss: 1.8461 - val_acc: 0.4108\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.41085, saving model to base_model.h5\n",
            "Epoch 2/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.7571 - acc: 0.7699 - val_loss: 2.4046 - val_acc: 0.5292\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.41085 to 0.52918, saving model to base_model.h5\n",
            "Epoch 3/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.5890 - acc: 0.8446 - val_loss: 1.9882 - val_acc: 0.5253\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.52918\n",
            "Epoch 4/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.4881 - acc: 0.8838 - val_loss: 1.1149 - val_acc: 0.6284\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.52918 to 0.62840, saving model to base_model.h5\n",
            "Epoch 5/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.4277 - acc: 0.9129 - val_loss: 1.8115 - val_acc: 0.5875\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.62840\n",
            "Epoch 6/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.3951 - acc: 0.9217 - val_loss: 0.8179 - val_acc: 0.7374\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.62840 to 0.73735, saving model to base_model.h5\n",
            "Epoch 7/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.3654 - acc: 0.9337 - val_loss: 0.8235 - val_acc: 0.7296\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.73735\n",
            "Epoch 8/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.3363 - acc: 0.9421 - val_loss: 1.7059 - val_acc: 0.6167\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.73735\n",
            "Epoch 9/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.3178 - acc: 0.9496 - val_loss: 0.9686 - val_acc: 0.7412\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0008500000403728336.\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.73735 to 0.74125, saving model to base_model.h5\n",
            "Epoch 10/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.2897 - acc: 0.9593 - val_loss: 0.6977 - val_acc: 0.7821\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.74125 to 0.78210, saving model to base_model.h5\n",
            "Epoch 11/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.2777 - acc: 0.9622 - val_loss: 0.7574 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.78210\n",
            "Epoch 12/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.2687 - acc: 0.9638 - val_loss: 0.6973 - val_acc: 0.7792\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.78210\n",
            "Epoch 13/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.2505 - acc: 0.9692 - val_loss: 0.7969 - val_acc: 0.7918\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.78210 to 0.79183, saving model to base_model.h5\n",
            "Epoch 14/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.2469 - acc: 0.9708 - val_loss: 0.9166 - val_acc: 0.7422\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.79183\n",
            "Epoch 15/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.2369 - acc: 0.9734 - val_loss: 1.3562 - val_acc: 0.7023\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0007225000590551645.\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.79183\n",
            "Epoch 16/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.2247 - acc: 0.9771 - val_loss: 0.5500 - val_acc: 0.8667\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.79183 to 0.86673, saving model to base_model.h5\n",
            "Epoch 17/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.2146 - acc: 0.9809 - val_loss: 0.5984 - val_acc: 0.8706\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.86673 to 0.87062, saving model to base_model.h5\n",
            "Epoch 18/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.2076 - acc: 0.9816 - val_loss: 0.5402 - val_acc: 0.8337\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.87062\n",
            "Epoch 19/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.2023 - acc: 0.9826 - val_loss: 0.5612 - val_acc: 0.8575\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.87062\n",
            "Epoch 20/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1999 - acc: 0.9820 - val_loss: 0.6414 - val_acc: 0.8171\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.87062\n",
            "Epoch 21/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1954 - acc: 0.9847 - val_loss: 0.6520 - val_acc: 0.8298\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0006141250254586339.\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.87062\n",
            "Epoch 22/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1858 - acc: 0.9863 - val_loss: 0.6311 - val_acc: 0.8103\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.87062\n",
            "Epoch 23/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1834 - acc: 0.9870 - val_loss: 0.6033 - val_acc: 0.8696\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.87062\n",
            "Epoch 24/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1829 - acc: 0.9870 - val_loss: 0.7691 - val_acc: 0.8268\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0005220062914304435.\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.87062\n",
            "Epoch 25/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1725 - acc: 0.9896 - val_loss: 0.7797 - val_acc: 0.8356\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.87062\n",
            "Epoch 26/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1707 - acc: 0.9904 - val_loss: 0.5129 - val_acc: 0.8872\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.87062 to 0.88716, saving model to base_model.h5\n",
            "Epoch 27/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1689 - acc: 0.9902 - val_loss: 0.7296 - val_acc: 0.8171\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.88716\n",
            "Epoch 28/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1684 - acc: 0.9903 - val_loss: 0.5131 - val_acc: 0.8959\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.88716 to 0.89591, saving model to base_model.h5\n",
            "Epoch 29/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1670 - acc: 0.9897 - val_loss: 0.6268 - val_acc: 0.8502\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00044370535761117935.\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.89591\n",
            "Epoch 30/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1604 - acc: 0.9925 - val_loss: 0.5140 - val_acc: 0.8911\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.89591\n",
            "Epoch 31/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1582 - acc: 0.9922 - val_loss: 0.5900 - val_acc: 0.8638\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.89591\n",
            "Epoch 32/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1561 - acc: 0.9927 - val_loss: 0.5510 - val_acc: 0.8959\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00037714955396950245.\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.89591\n",
            "Epoch 33/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1561 - acc: 0.9927 - val_loss: 0.5288 - val_acc: 0.8969\n",
            "\n",
            "Epoch 00033: val_acc improved from 0.89591 to 0.89689, saving model to base_model.h5\n",
            "Epoch 34/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1535 - acc: 0.9935 - val_loss: 0.5601 - val_acc: 0.8930\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.89689\n",
            "Epoch 35/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1547 - acc: 0.9930 - val_loss: 1.0186 - val_acc: 0.7763\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0003205771208740771.\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.89689\n",
            "Epoch 36/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1478 - acc: 0.9945 - val_loss: 0.6272 - val_acc: 0.8774\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.89689\n",
            "Epoch 37/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1476 - acc: 0.9955 - val_loss: 0.5479 - val_acc: 0.8879\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.89689\n",
            "Epoch 38/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1452 - acc: 0.9957 - val_loss: 0.5034 - val_acc: 0.9027\n",
            "\n",
            "Epoch 00038: val_acc improved from 0.89689 to 0.90272, saving model to base_model.h5\n",
            "Epoch 39/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1445 - acc: 0.9957 - val_loss: 0.6445 - val_acc: 0.8901\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.90272\n",
            "Epoch 40/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1440 - acc: 0.9955 - val_loss: 0.5769 - val_acc: 0.8891\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.90272\n",
            "Epoch 41/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1442 - acc: 0.9956 - val_loss: 0.6141 - val_acc: 0.8784\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0002724905527429655.\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.90272\n",
            "Epoch 42/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1415 - acc: 0.9959 - val_loss: 0.6126 - val_acc: 0.8696\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.90272\n",
            "Epoch 43/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1409 - acc: 0.9964 - val_loss: 0.6111 - val_acc: 0.8765\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.90272\n",
            "Epoch 44/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1394 - acc: 0.9968 - val_loss: 0.5520 - val_acc: 0.9115\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00023161696735769509.\n",
            "\n",
            "Epoch 00044: val_acc improved from 0.90272 to 0.91148, saving model to base_model.h5\n",
            "Epoch 45/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1413 - acc: 0.9954 - val_loss: 0.5397 - val_acc: 0.9047\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.91148\n",
            "Epoch 46/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1396 - acc: 0.9959 - val_loss: 0.4860 - val_acc: 0.9066\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.91148\n",
            "Epoch 47/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1385 - acc: 0.9964 - val_loss: 0.5965 - val_acc: 0.8804\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.91148\n",
            "Epoch 48/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1364 - acc: 0.9965 - val_loss: 0.5199 - val_acc: 0.8988\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.91148\n",
            "Epoch 49/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1382 - acc: 0.9965 - val_loss: 0.5363 - val_acc: 0.8930\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00019687442472786642.\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.91148\n",
            "Epoch 50/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1378 - acc: 0.9971 - val_loss: 0.6939 - val_acc: 0.8638\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.91148\n",
            "Epoch 51/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1387 - acc: 0.9962 - val_loss: 0.6641 - val_acc: 0.8745\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.91148\n",
            "Epoch 52/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1351 - acc: 0.9973 - val_loss: 0.5685 - val_acc: 0.9008\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.00016734325545257888.\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.91148\n",
            "Epoch 53/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1330 - acc: 0.9981 - val_loss: 0.5458 - val_acc: 0.8979\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.91148\n",
            "Epoch 54/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1364 - acc: 0.9968 - val_loss: 0.5976 - val_acc: 0.8930\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.91148\n",
            "Epoch 55/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1358 - acc: 0.9967 - val_loss: 0.5648 - val_acc: 0.8851\n",
            "\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.00014224176775314845.\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.91148\n",
            "Epoch 56/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1327 - acc: 0.9979 - val_loss: 0.5754 - val_acc: 0.8940\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.91148\n",
            "Epoch 57/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1339 - acc: 0.9971 - val_loss: 0.6019 - val_acc: 0.8872\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.91148\n",
            "Epoch 58/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1323 - acc: 0.9978 - val_loss: 0.5939 - val_acc: 0.8852\n",
            "\n",
            "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.00012090550444554538.\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.91148\n",
            "Epoch 59/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1321 - acc: 0.9978 - val_loss: 0.5956 - val_acc: 0.8842\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.91148\n",
            "Epoch 60/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1325 - acc: 0.9977 - val_loss: 0.5946 - val_acc: 0.8891\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.91148\n",
            "Epoch 61/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1349 - acc: 0.9966 - val_loss: 0.5632 - val_acc: 0.8979\n",
            "\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.00010276967877871357.\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.91148\n",
            "Epoch 62/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1341 - acc: 0.9969 - val_loss: 0.5298 - val_acc: 0.9115\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.91148\n",
            "Epoch 63/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1324 - acc: 0.9976 - val_loss: 0.5829 - val_acc: 0.8842\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.91148\n",
            "Epoch 64/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1322 - acc: 0.9976 - val_loss: 0.4946 - val_acc: 0.9076\n",
            "\n",
            "Epoch 00064: ReduceLROnPlateau reducing learning rate to 8.735422634345013e-05.\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.91148\n",
            "Epoch 65/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1332 - acc: 0.9973 - val_loss: 0.6169 - val_acc: 0.8833\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.91148\n",
            "Epoch 66/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1291 - acc: 0.9983 - val_loss: 0.4630 - val_acc: 0.9212\n",
            "\n",
            "Epoch 00066: val_acc improved from 0.91148 to 0.92121, saving model to base_model.h5\n",
            "Epoch 67/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1307 - acc: 0.9975 - val_loss: 0.4762 - val_acc: 0.9125\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.92121\n",
            "Epoch 68/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1324 - acc: 0.9974 - val_loss: 0.5594 - val_acc: 0.8911\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.92121\n",
            "Epoch 69/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1309 - acc: 0.9974 - val_loss: 0.5653 - val_acc: 0.8949\n",
            "\n",
            "Epoch 00069: ReduceLROnPlateau reducing learning rate to 7.425108960887882e-05.\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.92121\n",
            "Epoch 70/70\n",
            "749/749 [==============================] - 27s 36ms/step - loss: 0.1309 - acc: 0.9977 - val_loss: 0.5975 - val_acc: 0.8891\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.92121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeQ6tt-mwUW3",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's download the best model to our 'Capstone' folder in GoogleDrive:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA1xKG_gwVP4",
        "colab_type": "code",
        "outputId": "e84ef81b-f693-42b1-e822-aadfd1cbef7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "source = './base_model.h5'\n",
        "dest = 'gdrive/My Drive/Capstone/base_model_C1_C2-3_092121.h5'\n",
        "shutil.copyfile(source, dest)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gdrive/My Drive/Capstone/base_model_C1_C2-3_092121.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRz_1oJzJRFH",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's download the training history to a local file:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5Y5GUUhcyAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k,v in history.history.items():\n",
        "  history.history[k] = str(v)\n",
        "\n",
        "with open('history_dict.json', 'w') as f:\n",
        "    json.dump(history.history, f)\n",
        "\n",
        "try:\n",
        "    time.sleep(3) # To avoid warning when downloading various files at once\n",
        "    files.download('history_dict.json')\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxEpvYPVcyUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#uploaded = files.upload()\n",
        "with open('history_dict.json') as f:\n",
        "    history_dict = json.load(f)\n",
        "    \n",
        "for k,v in history_dict.items():\n",
        "  history_dict[k] = json.loads(v)\n",
        "\n",
        "history_df = pd.DataFrame(history_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eAC27OxKOLY",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's plot training and validation loss vs epochs:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFs4JAkn-vtq",
        "colab_type": "code",
        "outputId": "60dc658c-bf06-45d8-daa7-4217d3466bdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "loss = history_df[['loss','val_loss']]\n",
        "loss.columns = ['train_loss', 'val_loss']\n",
        "loss.plot(figsize=(10, 6), title='Loss vs epochs')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0e1adfc400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAAF1CAYAAAD80H5/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSFAqg-gKlR0",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's plot training and validation accuracy vs epochs:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zux8aIdQN7zN",
        "colab_type": "code",
        "outputId": "ce35dba9-49cf-4385-c7a6-9f865d43f391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "acc = history_df[['val_acc','acc']]\n",
        "acc.columns = ['val_acc', 'train_acc']\n",
        "acc.plot(figsize=(10, 6), title='Val acc & Train acc vs epochs')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0e1b5f50f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF1CAYAAADMXG9eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXBmwA6wKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8RiTO3yLDXy",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's plot learning rate vs epochs:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2ggKlccOAHA",
        "colab_type": "code",
        "outputId": "01895149-f7b0-4476-c15d-b5197a181cca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "lr = history_df[['lr']]\n",
        "lr.plot(figsize=(10, 6), title='Learning rate vs epochs')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0e1b56c198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAF1CAYAAABChiYiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCIIZAABAivi/X4P2LYTT91sAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEjAnQK9Mhfy",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's evaluate the best model, on the validation set and compute relevant metrics:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc4dm0GKcCb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#uploaded = files.upload()\n",
        "res_cnn.load_weights('base_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4QyhhkriB7C",
        "colab_type": "code",
        "outputId": "5e8b082a-83a8-4e19-b8e5-80f6df22fc7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "## fully balanced training\n",
        "## Rot every 45\n",
        "## 'C1' vs 'C2-3' vs 'all_other' - Downsampling training\n",
        "\n",
        "## 128x128, stride_60,\n",
        "##min_pos_pix_1250, mivalpos_1024\n",
        "## ReduceLROnPlateau(monitor='val_loss'... )\n",
        "## train_batch_32, opt_RMSprop, Kernel_3x3:\n",
        "\n",
        "X, y_true = next(val_generator)\n",
        "y_pred = res_cnn.predict(X)\n",
        "for i in range(1, len(val_generator)):\n",
        "  X, y = next(val_generator)\n",
        "  y_true = np.vstack((y_true, y))\n",
        "  y_pred = np.vstack((y_pred, res_cnn.predict(X)))\n",
        "\n",
        "y_true = np.argmax(y_true, axis=1)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "val_acc = accuracy_score(y_true, y_pred)\n",
        "#roc_auc = roc_auc_score(y_true, y_pred)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "class_names = [k for k in val_generator.class_indices]\n",
        "c_report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "\n",
        "print('\\nval_acc:\\n', val_acc)\n",
        "print('\\nConfusion Matrix:\\n', cm)\n",
        "print('\\nClassification Report:\\n', c_report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_acc:\n",
            " 0.8983516483516484\n",
            "\n",
            "Confusion Matrix:\n",
            " [[362   1   1]\n",
            " [ 92 259  13]\n",
            " [  2   2 360]]\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "       C1_pos       0.79      0.99      0.88       364\n",
            "     C2-3_pos       0.99      0.71      0.83       364\n",
            "all_other_pos       0.96      0.99      0.98       364\n",
            "\n",
            "     accuracy                           0.90      1092\n",
            "    macro avg       0.91      0.90      0.90      1092\n",
            " weighted avg       0.91      0.90      0.90      1092\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR51ObEzTJ9R",
        "colab_type": "text"
      },
      "source": [
        "## **Let's evaluate the best model, when loaded from GoogleDrive:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPCfAuGOH1gF",
        "colab_type": "code",
        "outputId": "e65ecdbf-bc5e-44c2-87e9-4d30b4cf876d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = '18De1DbqyxD1JlNpue6LIUZXd73VgAN-f' # final model C1 vs C2-3 vs all_other\n",
        "\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile(downloaded['title'])\n",
        "print('Downloaded content: \"{}\"'.format(downloaded['title']))\n",
        "print('Root dir content: {}'.format(os.listdir()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded content: \"base_model_C1_C2-3_092121.h5\"\n",
            "Root dir content: ['.config', 'Patches', 'gdrive', 'adc.json', 'base_model.h5', 'base_model_C1_C2-3_092121.h5', 'history_dict.json', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQATUr-sUBpu",
        "colab_type": "text"
      },
      "source": [
        "#### **Results when creating a fresh network and only weights are loaded:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFwUtvGBSiBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = list(iter(train_generator.class_indices))\n",
        "n_classes = len(classes)\n",
        "layers_per_block = [4, 4, 4, 4] #18 layers total with first conv and last FC\n",
        "n_filters = [64, 128, 256, 512]\n",
        "kernel_sizes = [(3,3), (3,3), (3,3), (3,3)]\n",
        "l2_reg = 0.1\n",
        "optimizer = RMSprop # Adamax, RMSprop, Adam (No: Nadam, SGD)\n",
        "lr = 1e-3\n",
        "decay = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "res_cnn = make_resnet(img_size, n_classes, layers_per_block, n_filters,\n",
        "                       kernel_sizes, l2_reg, optimizer, lr, decay, momentum)\n",
        "\n",
        "res_cnn.load_weights(downloaded['title'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOkYO5hR_eaO",
        "colab_type": "code",
        "outputId": "d00eed44-aeb6-425a-9706-c2c19f3cbd3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "## fresh network, loading weights\n",
        "\n",
        "X, y_true = next(val_generator)\n",
        "y_pred = res_cnn.predict(X)\n",
        "for i in range(1, len(val_generator)):\n",
        "  X, y = next(val_generator)\n",
        "  y_true = np.vstack((y_true, y))\n",
        "  y_pred = np.vstack((y_pred, res_cnn.predict(X)))\n",
        "\n",
        "y_true = np.argmax(y_true, axis=1)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "val_acc = accuracy_score(y_true, y_pred)\n",
        "#roc_auc = roc_auc_score(y_true, y_pred)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "class_names = [k for k in val_generator.class_indices]\n",
        "c_report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "\n",
        "print('\\nval_acc:\\n', val_acc)\n",
        "print('\\nConfusion Matrix:\\n', cm)\n",
        "print('\\nClassification Report:\\n', c_report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_acc:\n",
            " 0.8983516483516484\n",
            "\n",
            "Confusion Matrix:\n",
            " [[362   1   1]\n",
            " [ 92 259  13]\n",
            " [  2   2 360]]\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "       C1_pos       0.79      0.99      0.88       364\n",
            "     C2-3_pos       0.99      0.71      0.83       364\n",
            "all_other_pos       0.96      0.99      0.98       364\n",
            "\n",
            "     accuracy                           0.90      1092\n",
            "    macro avg       0.91      0.90      0.90      1092\n",
            " weighted avg       0.91      0.90      0.90      1092\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrsN8yvNUXPf",
        "colab_type": "text"
      },
      "source": [
        "#### **Results when loading the entire model (not just weights):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkLv3rnDJIpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_model = load_model(downloaded['title'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGdaZIe8_pyn",
        "colab_type": "code",
        "outputId": "41a76a9b-9d83-48dc-e6e3-1d6f7038e284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "## entire model loaded from GoogleDrive (not just weights)\n",
        "\n",
        "X, y_true = next(val_generator)\n",
        "y_pred = final_model.predict(X)\n",
        "for i in range(1, len(val_generator)):\n",
        "  X, y = next(val_generator)\n",
        "  y_true = np.vstack((y_true, y))\n",
        "  y_pred = np.vstack((y_pred, final_model.predict(X)))\n",
        "\n",
        "y_true = np.argmax(y_true, axis=1)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "val_acc = accuracy_score(y_true, y_pred)\n",
        "#roc_auc = roc_auc_score(y_true, y_pred)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "class_names = [k for k in val_generator.class_indices]\n",
        "c_report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "\n",
        "print('\\nval_acc:\\n', val_acc)\n",
        "print('\\nConfusion Matrix:\\n', cm)\n",
        "print('\\nClassification Report:\\n', c_report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_acc:\n",
            " 0.8983516483516484\n",
            "\n",
            "Confusion Matrix:\n",
            " [[362   1   1]\n",
            " [ 92 259  13]\n",
            " [  2   2 360]]\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "       C1_pos       0.79      0.99      0.88       364\n",
            "     C2-3_pos       0.99      0.71      0.83       364\n",
            "all_other_pos       0.96      0.99      0.98       364\n",
            "\n",
            "     accuracy                           0.90      1092\n",
            "    macro avg       0.91      0.90      0.90      1092\n",
            " weighted avg       0.91      0.90      0.90      1092\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}