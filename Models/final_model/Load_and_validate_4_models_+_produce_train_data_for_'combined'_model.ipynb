{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Load and validate 4 models + produce train data for 'combined' model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYUPhq76Zm35",
        "colab_type": "text"
      },
      "source": [
        "###  <span style=\"color:red\">**This Notebook can be run from Google Colab:**</span>\n",
        "\n",
        "https://colab.research.google.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMveENLYCJA9",
        "colab_type": "text"
      },
      "source": [
        "# **<span style=\"color:red\">Background and motivation:</span>**\n",
        "\n",
        "#### First, we trained a model using positive patches only (only patches actually containing regions corresponding to growing bacterial colonies in the petri-dish), to specifically differentiate among the 8 bacterial species in our dataset. From the confusion matrix of that model, we could see that the model is having difficulty to differentiate between classes 'C1' and 'C2-3' and between classes 'C4-7' and 'C5'.\n",
        "\n",
        "#### As a next step, we then trained a model to specifically learn to differentiate 'C1' vs 'C2-3' vs 'all_other' classes. This is a model with 3 classes only. \n",
        "\n",
        "#### Similarly, we also trained a model to specifically learn to differentiate 'C4-7' vs 'C5' vs 'all_other' classes.\n",
        "\n",
        "#### The last model we trained was a model to specifically differentiate between positive bacterial colony patches (of any class) and negative patches (either petri-dish background, petri-dish border or white image background). For this, we just combined all positive patches (regardless of the bacterial species) in a single 'positive' class and all negative patches in a single 'negative' class.  \n",
        "\n",
        "#### Now, we have 4 models, the first one producing 8 predicted probabilities (one for each baterial species), the second one producing 3 predicted probabilities ('C1', 'C2-3', 'all_other'), the third one also producing 3 predicted probabilities ('C4-7', 'C5', 'all_other') and the fourth one producing 2 predicted probabilities (positive_patch, negative_patch), for a total of 16 predicted probabilities.\n",
        "\n",
        "#### As a final step, before testing our model, we want to **combine** our models. For this, we will try using the 16 predicted probabilities as features and train a SVM or other simple classification model, to learn to predict either negative or the correct bacterial species, from the probabilities produced by the 4 models above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yom3XAA8ZnSA",
        "colab_type": "code",
        "outputId": "94ebe850-5102-442b-9dd9-5dbea2f9bcf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import json\n",
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, \\\n",
        "                            classification_report, balanced_accuracy_score\n",
        "\n",
        "# Import PyDrive and associated libraries (to connect with GoogleDrive):\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Mount my GoogleDrive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# disable warnings\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DItexGaqdfEA",
        "colab_type": "text"
      },
      "source": [
        "### **Check if we are using GPU:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn7AYx74dNq6",
        "colab_type": "code",
        "outputId": "52ae91d4-dfe7-4156-c841-bf13c80f567f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import backend as K\n",
        "if K.backend() == \"tensorflow\":\n",
        "    import tensorflow as tf\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "    if device_name == '':\n",
        "        device_name = \"None\"\n",
        "    print('Using TensorFlow version:', tf.__version__, ', GPU:', device_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow version: 1.15.0 , GPU: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQXfhMrHlqrz",
        "colab_type": "text"
      },
      "source": [
        "### **Download Validation ('Control') patches from GoogleDrive:**\n",
        "\n",
        "### This dataset contains 9 classes in total (8 bacterial species + negative patches).\n",
        "\n",
        "#### *Validation Patches were augmented with Patch_Generator, using 'stride=22' and rotations every 20 degrees until a full lap. Patches were then balanced by downsampling majority classes so we can compare accuracy of the model.*\n",
        "\n",
        "###  **NOTE: Validation patches were generated from original, non-preprocessed images. In this way, we will ensure our model perform well at testing time when pre-processing may not be feasible. As example, being able to create masks/image annotation may not be feasible on testing data.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACvUlEucnnY0",
        "colab_type": "code",
        "outputId": "49b835bb-f7c5-42e8-fc9c-43a1993e6374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#\"Control_for_final_training_9_classes_128_vs22_minval_1024_rotate_every_20_full_balance.zip\":\n",
        "file_id = '1bebJEgFoWq04jX-6ZfqZPxAoxH5j4G9e' # Control only, rotate every 20, full balance\n",
        "\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile(downloaded['title'])\n",
        "print('Downloaded content: \"{}\"'.format(downloaded['title']))\n",
        "print('Root dir content: {}'.format(os.listdir()))\n",
        "patches_zip = downloaded['title']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded content: \"Control_for_final_training_9_classes_128_vs22_minval_1024_rotate_every_20_full_balance.zip\"\n",
            "Root dir content: ['.config', 'gdrive', 'adc.json', 'Control_for_final_training_9_classes_128_vs22_minval_1024_rotate_every_20_full_balance.zip', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSdGQav-qEJM",
        "colab_type": "text"
      },
      "source": [
        "### **Unzip the Validation ('Control') patches:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP1-2THkn47w",
        "colab_type": "code",
        "outputId": "a2cc50b7-834d-455b-9619-43091dbd7f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Remove 'Patches' dir if it already exists\n",
        "if 'Patches' in os.listdir():\n",
        "  shutil.rmtree('./Patches')\n",
        "with zipfile.ZipFile(patches_zip,\"r\") as zip:\n",
        "    zip.extractall()\n",
        "os.remove(downloaded['title'])\n",
        "print('Root dir content: {}'.format(os.listdir()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root dir content: ['.config', 'Patches', 'gdrive', 'adc.json', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd7htfD3rJ6V",
        "colab_type": "text"
      },
      "source": [
        "### **Let's count patches by type and class:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH5rSidQbggC",
        "colab_type": "code",
        "outputId": "4fcac364-8456-4c67-95ef-66e7bedcbd44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "classes = ['C1','C2-3','C4-7','C5','C6','C8','C9','C10','neg']\n",
        "class_weights = {} # empty dictionary to store class weights\n",
        "\n",
        "grand_total = 0\n",
        "for type_ in ['Serial', 'Control', 'Streak']:\n",
        "    print(\"\\nTotal '{}' Patches per location:\".format(type_))\n",
        "    n_type = 0\n",
        "    class_weights[type_] = {} # nested empty dictionary to store class weights\n",
        "    for cls in classes:\n",
        "        if cls != 'neg':\n",
        "            pos_folder = './Patches/{}/{}_pos'.format(type_,cls)\n",
        "        else:\n",
        "            pos_folder = './Patches/{}/{}'.format(type_,cls)\n",
        "        n_pos = len(os.listdir(pos_folder))\n",
        "        n_type += n_pos\n",
        "        #print(pos_folder, n_pos)\n",
        "        print('total_{}: {}'.format(cls,n_pos))\n",
        "        class_weights[type_]['{}'.format(cls)] = 1/n_pos if n_pos else 0\n",
        "    print('Total {}: {}'.format(type_,n_type))\n",
        "    for loc in class_weights[type_].keys():\n",
        "        class_weights[type_][loc] *= n_type\n",
        "    grand_total += n_type\n",
        "print('\\nGRAND TOTAL: {}'.format(grand_total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total 'Serial' Patches per location:\n",
            "total_C1: 0\n",
            "total_C2-3: 0\n",
            "total_C4-7: 0\n",
            "total_C5: 0\n",
            "total_C6: 0\n",
            "total_C8: 0\n",
            "total_C9: 0\n",
            "total_C10: 0\n",
            "total_neg: 0\n",
            "Total Serial: 0\n",
            "\n",
            "Total 'Control' Patches per location:\n",
            "total_C1: 6610\n",
            "total_C2-3: 6610\n",
            "total_C4-7: 6610\n",
            "total_C5: 6610\n",
            "total_C6: 6610\n",
            "total_C8: 6610\n",
            "total_C9: 6610\n",
            "total_C10: 6610\n",
            "total_neg: 6610\n",
            "Total Control: 59490\n",
            "\n",
            "Total 'Streak' Patches per location:\n",
            "total_C1: 0\n",
            "total_C2-3: 0\n",
            "total_C4-7: 0\n",
            "total_C5: 0\n",
            "total_C6: 0\n",
            "total_C8: 0\n",
            "total_C9: 0\n",
            "total_C10: 0\n",
            "total_neg: 0\n",
            "Total Streak: 0\n",
            "\n",
            "GRAND TOTAL: 59490\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuEOQpfgfXdX",
        "colab_type": "text"
      },
      "source": [
        "#### **Just in case we have imbalanced training data, we have set different class weights to give more importance to the minority classes:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKAnEaI8fYn7",
        "colab_type": "code",
        "outputId": "c89aa891-e47e-4dd3-fb2b-c01ece90e20c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print('Class Weights:', str(json.dumps(class_weights['Control'], indent=2, default=str)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class Weights: {\n",
            "  \"C1\": 9.0,\n",
            "  \"C2-3\": 9.0,\n",
            "  \"C4-7\": 9.0,\n",
            "  \"C5\": 9.0,\n",
            "  \"C6\": 9.0,\n",
            "  \"C8\": 9.0,\n",
            "  \"C9\": 9.0,\n",
            "  \"C10\": 9.0,\n",
            "  \"neg\": 9.0\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNJ7lSKQpdOT",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's build the validation generator, using keras.preprocessing.image.ImageDataGenerator, rescaling image pixel values from [0,  255] to [0, 1]:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UANfdA6IUFKt",
        "colab_type": "code",
        "outputId": "beeb9550-f594-4f46-c52c-1e25528951a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c1_pos_folder = './Patches/Control/C1_pos'\n",
        "img = plt.imread(c1_pos_folder + '/' + os.listdir(c1_pos_folder)[:5][0])\n",
        "img_size = img.shape\n",
        "val_batch_size = 64\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "        './Patches/Control',\n",
        "        target_size=(img_size[0],img_size[1]),\n",
        "        batch_size=val_batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 59490 images belonging to 9 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ZzUGXvwqUn",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's check what is the data generators' index for each class:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSHQNC2awn84",
        "colab_type": "code",
        "outputId": "146c281c-2b3d-424f-84e4-e817837d23a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "print('validation_generator.class_indices:', str(json.dumps(val_generator.class_indices, indent=2, default=str)))\n",
        "\n",
        "print('Our class weights:', str(json.dumps(class_weights['Control'], indent=2, default=str)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation_generator.class_indices: {\n",
            "  \"C10_pos\": 0,\n",
            "  \"C1_pos\": 1,\n",
            "  \"C2-3_pos\": 2,\n",
            "  \"C4-7_pos\": 3,\n",
            "  \"C5_pos\": 4,\n",
            "  \"C6_pos\": 5,\n",
            "  \"C8_pos\": 6,\n",
            "  \"C9_pos\": 7,\n",
            "  \"neg\": 8\n",
            "}\n",
            "Our class weights: {\n",
            "  \"C1\": 9.0,\n",
            "  \"C2-3\": 9.0,\n",
            "  \"C4-7\": 9.0,\n",
            "  \"C5\": 9.0,\n",
            "  \"C6\": 9.0,\n",
            "  \"C8\": 9.0,\n",
            "  \"C9\": 9.0,\n",
            "  \"C10\": 9.0,\n",
            "  \"neg\": 9.0\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKgykGT1e46K",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's set up the class weights in correct order:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-wypcdrfBGx",
        "colab_type": "code",
        "outputId": "684518ed-e048-4c6f-a149-6fb1608da497",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "control_pos_weights = [class_weights['Control']['C10']] # C10 has index 0\n",
        "for cls in classes:\n",
        "    if cls == 'C10': continue\n",
        "    control_pos_weights.append(class_weights['Control']['{}'.format(cls)])\n",
        "print('original class weights dictionary:')\n",
        "print(str(json.dumps(class_weights['Control'], indent=2, default=str)))\n",
        "print('class weights for generator, re-arranging indexes:')\n",
        "print(str(json.dumps(control_pos_weights, indent=2, default=str)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original class weights dictionary:\n",
            "{\n",
            "  \"C1\": 9.0,\n",
            "  \"C2-3\": 9.0,\n",
            "  \"C4-7\": 9.0,\n",
            "  \"C5\": 9.0,\n",
            "  \"C6\": 9.0,\n",
            "  \"C8\": 9.0,\n",
            "  \"C9\": 9.0,\n",
            "  \"C10\": 9.0,\n",
            "  \"neg\": 9.0\n",
            "}\n",
            "class weights for generator, re-arranging indexes:\n",
            "[\n",
            "  9.0,\n",
            "  9.0,\n",
            "  9.0,\n",
            "  9.0,\n",
            "  9.0,\n",
            "  9.0,\n",
            "  9.0,\n",
            "  9.0,\n",
            "  9.0\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRz_1oJzJRFH",
        "colab_type": "text"
      },
      "source": [
        "### **Let's download the 4 final models from GoogleDrive:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5Y5GUUhcyAt",
        "colab_type": "code",
        "outputId": "6aca328b-e074-4ba7-e0f3-476930c77e88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "eight_classes_id = '1w0u_EKaSG8zkMRtYkNjFd3IOnR3IpQsJ' # model_8_classes_0.8465\n",
        "C1_C2_3_id = '18De1DbqyxD1JlNpue6LIUZXd73VgAN-f' # model C1 vs C2-3 vs all_other\n",
        "C4_7_C5_id = '1-4e6W-yR13q3ckpgo8O9QVMTncWITHwg' # model_C4-7_vs_C5_vs_all-other\n",
        "pos_vs_neg_id = '1-BxPnguFXE7PHmzKadW0AnwWO9VqTywR' # model pos-neg\n",
        "\n",
        "files_ids_dict = {'model_eight_classes': eight_classes_id,\n",
        "                  'model_C1_C2_3': C1_C2_3_id,\n",
        "                  'model_C4_7_C5': C4_7_C5_id,\n",
        "                  'model_pos_vs_neg': pos_vs_neg_id}\n",
        "\n",
        "models_names_dict = {}\n",
        "for model_name, file_id in files_ids_dict.items():\n",
        "    downloaded = drive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(downloaded['title'])\n",
        "    print('Downloaded content: \"{}\"'.format(downloaded['title']))\n",
        "    print('Root dir content: {}\\n'.format(os.listdir()))\n",
        "    models_names_dict[model_name] = downloaded['title']\n",
        "\n",
        "print('models_names_dict:', str(json.dumps(models_names_dict, indent=2, default=str)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded content: \"model_8_classes_08465.h5\"\n",
            "Root dir content: ['.config', 'Patches', 'gdrive', 'adc.json', 'model_8_classes_08465.h5', 'sample_data']\n",
            "\n",
            "Downloaded content: \"model_C1_C2-3_08983.h5\"\n",
            "Root dir content: ['.config', 'Patches', 'gdrive', 'model_C1_C2-3_08983.h5', 'adc.json', 'model_8_classes_08465.h5', 'sample_data']\n",
            "\n",
            "Downloaded content: \"model_C4-7_C5_083.h5\"\n",
            "Root dir content: ['.config', 'Patches', 'gdrive', 'model_C1_C2-3_08983.h5', 'adc.json', 'model_C4-7_C5_083.h5', 'model_8_classes_08465.h5', 'sample_data']\n",
            "\n",
            "Downloaded content: \"model_pos_neg_09973.h5\"\n",
            "Root dir content: ['.config', 'Patches', 'gdrive', 'model_C1_C2-3_08983.h5', 'adc.json', 'model_C4-7_C5_083.h5', 'model_pos_neg_09973.h5', 'model_8_classes_08465.h5', 'sample_data']\n",
            "\n",
            "models_names_dict: {\n",
            "  \"model_eight_classes\": \"model_8_classes_08465.h5\",\n",
            "  \"model_C1_C2_3\": \"model_C1_C2-3_08983.h5\",\n",
            "  \"model_C4_7_C5\": \"model_C4-7_C5_083.h5\",\n",
            "  \"model_pos_vs_neg\": \"model_pos_neg_09973.h5\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEjAnQK9Mhfy",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's load all 4 models from downloaded files:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH-tFKnoMjpw",
        "colab_type": "code",
        "outputId": "e274c0a5-8c38-4a9d-f3d9-a5e982c63f1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "eight_classes_model = load_model(models_names_dict['model_eight_classes'])\n",
        "#eight_classes_model.summary() # summarize model."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O2mjdcllx4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C1_C2_3_model = load_model(models_names_dict['model_C1_C2_3'])\n",
        "#C1_C2_3_model.summary() # summarize model."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UewyWEJOl_Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C4_7_C5_model = load_model(models_names_dict['model_C4_7_C5'])\n",
        "#C4_7_C5_model.summary() # summarize model."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIpwzCLVmRuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_vs_neg_model = load_model(models_names_dict['model_pos_vs_neg'])\n",
        "#pos_vs_neg_C5_model.summary() # summarize model."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oXacV9WmuGf",
        "colab_type": "text"
      },
      "source": [
        "## **Let's *'evaluate'* the models, just to make sure they loaded correctly:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r50HcLL8siHo",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's evaluate '8 classes' model on the validation set:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1GmvrSm1XEg",
        "colab_type": "code",
        "outputId": "5ddac9ab-d1ea-4fe7-c511-77782bc4c1ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "y_true = val_generator.classes\n",
        "eight_classes_scores = eight_classes_model.predict_generator(val_generator)\n",
        "y_pred = np.argmax(eight_classes_scores, axis=1)\n",
        "\n",
        "# remove 'neg' class (index = 8):\n",
        "y_true, y_pred = y_true[(y_true != 8)], y_pred[(y_true != 8)]\n",
        "\n",
        "val_acc = accuracy_score(y_true, y_pred)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "class_names = [k for k in val_generator.class_indices if k != 'neg']\n",
        "c_report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "\n",
        "print('\\nbalanced val_acc:\\n', val_acc)\n",
        "print('\\nConfusion Matrix:\\n', cm)\n",
        "print('\\nClassification Report:\\n', c_report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "balanced val_acc:\n",
            " 0.8236951588502269\n",
            "\n",
            "Confusion Matrix:\n",
            " [[6545   26   34    0    0    5    0    0]\n",
            " [  30 5824  657    0    0    1   98    0]\n",
            " [   0 2043 4269    0    0  107  187    4]\n",
            " [   0    0   48 5458 1048    2   54    0]\n",
            " [   0   14    0 3891 2696    0    9    0]\n",
            " [  54    1   61   47    5 6359   82    1]\n",
            " [   0  190    9  581   34    0 5796    0]\n",
            " [   0    0    0    0    0    0    0 6610]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     C10_pos       0.99      0.99      0.99      6610\n",
            "      C1_pos       0.72      0.88      0.79      6610\n",
            "    C2-3_pos       0.84      0.65      0.73      6610\n",
            "    C4-7_pos       0.55      0.83      0.66      6610\n",
            "      C5_pos       0.71      0.41      0.52      6610\n",
            "      C6_pos       0.98      0.96      0.97      6610\n",
            "      C8_pos       0.93      0.88      0.90      6610\n",
            "      C9_pos       1.00      1.00      1.00      6610\n",
            "\n",
            "    accuracy                           0.82     52880\n",
            "   macro avg       0.84      0.82      0.82     52880\n",
            "weighted avg       0.84      0.82      0.82     52880\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQDagJTEslfr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "**Let's remember the val_generator class indices (we will need them next):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddoNhhyOC_28",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "4bae1d9c-8b92-497c-d267-16bec1ba55cc"
      },
      "source": [
        "print('validation_generator.class_indices:', str(json.dumps(val_generator.class_indices, indent=2, default=str)))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation_generator.class_indices: {\n",
            "  \"C10_pos\": 0,\n",
            "  \"C1_pos\": 1,\n",
            "  \"C2-3_pos\": 2,\n",
            "  \"C4-7_pos\": 3,\n",
            "  \"C5_pos\": 4,\n",
            "  \"C6_pos\": 5,\n",
            "  \"C8_pos\": 6,\n",
            "  \"C9_pos\": 7,\n",
            "  \"neg\": 8\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OunhDhWzF_no",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's evaluate 'C1_C2_3' model on the validation set:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io-59ijjf6cu",
        "colab_type": "code",
        "outputId": "5a212afc-ce6d-48cf-9ec1-a88ee83c4ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "y_true = val_generator.classes\n",
        "C1_C2_3_scores = C1_C2_3_model.predict_generator(val_generator)\n",
        "y_pred = np.argmax(C1_C2_3_scores, axis=1)\n",
        "\n",
        "# remove 'neg' class (index = 8):\n",
        "y_true, y_pred = y_true[(y_true != 8)], y_pred[(y_true != 8)]\n",
        "\n",
        "# class indices for C1_C2_3_model are as follows:\n",
        "C1_C2_3_model_class_dict = {0:\"C1_pos\", 1:\"C2-3_pos\", 2:\"all_other_pos\"}\n",
        "\n",
        "# let's map y_pred class indices from 0,1,2 to 1,2,3 (same as val_generator):\n",
        "y_pred +=1\n",
        "\n",
        "# in y_true, convert classes != 'C1_pos'(1),'C2-3_pos'(2) into 'all_other'(3):\n",
        "y_true[(y_true!=1) & (y_true!=2)] = 3\n",
        "\n",
        "val_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "class_names = [v for v in C1_C2_3_model_class_dict.values()]\n",
        "c_report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "\n",
        "print('\\nbalanced val_acc:\\n', val_acc)\n",
        "print('\\nConfusion Matrix:\\n', cm)\n",
        "print('\\nClassification Report:\\n', c_report)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "balanced val_acc:\n",
            " 0.8449235165574046\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 6352   232    26]\n",
            " [ 1995  3890   725]\n",
            " [  271   312 39077]]\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "       C1_pos       0.74      0.96      0.83      6610\n",
            "     C2-3_pos       0.88      0.59      0.70      6610\n",
            "all_other_pos       0.98      0.99      0.98     39660\n",
            "\n",
            "     accuracy                           0.93     52880\n",
            "    macro avg       0.87      0.84      0.84     52880\n",
            " weighted avg       0.94      0.93      0.93     52880\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSiAK3tZJQOV",
        "colab_type": "text"
      },
      "source": [
        "**Again, just remember the val_generator class indices (we will need them next):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPxGtbtYHRtn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "aa3d062e-dfee-4cb2-870a-7f09c5ef6a3f"
      },
      "source": [
        "print('validation_generator.class_indices:', str(json.dumps(val_generator.class_indices, indent=2, default=str)))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation_generator.class_indices: {\n",
            "  \"C10_pos\": 0,\n",
            "  \"C1_pos\": 1,\n",
            "  \"C2-3_pos\": 2,\n",
            "  \"C4-7_pos\": 3,\n",
            "  \"C5_pos\": 4,\n",
            "  \"C6_pos\": 5,\n",
            "  \"C8_pos\": 6,\n",
            "  \"C9_pos\": 7,\n",
            "  \"neg\": 8\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyiG-qfFsmTt",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's evaluate 'C4-7_C5' model on the validation set:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoPWYldkkryC",
        "colab_type": "code",
        "outputId": "7d1b68de-2a3d-41b6-baf9-f9ce748133cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "y_true = val_generator.classes\n",
        "C4_7_C5_scores = C4_7_C5_model.predict_generator(val_generator)\n",
        "y_pred = np.argmax(C4_7_C5_scores, axis=1)\n",
        "\n",
        "# remove 'neg' class (index = 8):\n",
        "y_true, y_pred = y_true[(y_true != 8)], y_pred[(y_true != 8)]\n",
        "\n",
        "# class indices for C4-7_C5_model are as follows:\n",
        "C4_7_C5_model_class_dict = {0:\"C4-7_pos\", 1:\"C5_pos\",2: \"all_other_pos\"}\n",
        "\n",
        "# in y_true, convert classes != 'C4-7_pos'(3),'C5_pos'(4) into 'all_other'(2):\n",
        "y_true[(y_true!=3) & (y_true!=4)] = 2\n",
        "\n",
        "# now, let's map 'C4-7_pos' from 3 to 0 and 'C5_pos' from 4 to 1:\n",
        "y_true[y_true==3] = 0\n",
        "y_true[y_true==4] = 1\n",
        "\n",
        "val_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "class_names = [v for v in C4_7_C5_model_class_dict.values()]\n",
        "c_report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "\n",
        "print('\\nbalanced val_acc:\\n', val_acc)\n",
        "print('\\nConfusion Matrix:\\n', cm)\n",
        "print('\\nClassification Report:\\n', c_report)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "balanced val_acc:\n",
            " 0.82523953605648\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 3943  2506   161]\n",
            " [  499  6054    57]\n",
            " [ 1005   450 38205]]\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     C4-7_pos       0.72      0.60      0.65      6610\n",
            "       C5_pos       0.67      0.92      0.78      6610\n",
            "all_other_pos       0.99      0.96      0.98     39660\n",
            "\n",
            "     accuracy                           0.91     52880\n",
            "    macro avg       0.80      0.83      0.80     52880\n",
            " weighted avg       0.92      0.91      0.91     52880\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHdRLbjPJbit",
        "colab_type": "text"
      },
      "source": [
        "**Again, just remember the val_generator class indices (we will need them next):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofdp6x8EJHWI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "a66fa195-b877-41fe-efa8-1661e5a4d755"
      },
      "source": [
        "print('validation_generator.class_indices:', str(json.dumps(val_generator.class_indices, indent=2, default=str)))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation_generator.class_indices: {\n",
            "  \"C10_pos\": 0,\n",
            "  \"C1_pos\": 1,\n",
            "  \"C2-3_pos\": 2,\n",
            "  \"C4-7_pos\": 3,\n",
            "  \"C5_pos\": 4,\n",
            "  \"C6_pos\": 5,\n",
            "  \"C8_pos\": 6,\n",
            "  \"C9_pos\": 7,\n",
            "  \"neg\": 8\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5n4MADAsplY",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's evaluate 'pos_vs_neg' model on the validation set:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyIRU25UJeVj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "0b677f36-adaf-43eb-90da-a959d0881771"
      },
      "source": [
        "y_true = val_generator.classes\n",
        "pos_vs_neg_scores = pos_vs_neg_model.predict_generator(val_generator)\n",
        "y_pred = np.argmax(pos_vs_neg_scores, axis=1)\n",
        "\n",
        "# remove 'neg' class (index = 8):\n",
        "y_true, y_pred = y_true[(y_true != 8)], y_pred[(y_true != 8)]\n",
        "\n",
        "# class indices for C4-7_C5_model are as follows:\n",
        "pos_vs_neg_model_class_dict = {0:'neg', 1:'pos'}\n",
        "\n",
        "# in y_true, convert classes != 'neg'(8) to 1:\n",
        "y_true[y_true!=8] = 1\n",
        "\n",
        "# now, let's map 'neg' from 8 to 0:\n",
        "y_true[y_true==8] = 0\n",
        "\n",
        "val_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "class_names = [v for v in pos_vs_neg_model_class_dict.values()]\n",
        "c_report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "\n",
        "print('\\nbalanced val_acc:\\n', val_acc)\n",
        "print('\\nConfusion Matrix:\\n', cm)\n",
        "print('\\nClassification Report:\\n', c_report)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "balanced val_acc:\n",
            " 0.9990733736762482\n",
            "\n",
            "Confusion Matrix:\n",
            " [[    0     0]\n",
            " [   49 52831]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.00      0.00      0.00         0\n",
            "         pos       1.00      1.00      1.00     52880\n",
            "\n",
            "    accuracy                           1.00     52880\n",
            "   macro avg       0.50      0.50      0.50     52880\n",
            "weighted avg       1.00      1.00      1.00     52880\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q69R2-ZBuY9U",
        "colab_type": "text"
      },
      "source": [
        "### **All models performed as expected let's now produce a composite array of training data with the features being the total 16 scores produced by the 4 models and the target being the actual class from the 9 classes in the downloaded patches:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNCAHUyuLqo7",
        "colab_type": "text"
      },
      "source": [
        "#### **First, let's check the sizes to make sure they are correct:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARRyzIKguH6_",
        "colab_type": "code",
        "outputId": "108036b8-6200-4328-a075-d7318fdf3eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "y_true = val_generator.classes\n",
        "print('y_true.shape', y_true.shape)\n",
        "print('eight_classes_scores.shape', eight_classes_scores.shape)\n",
        "print('C1_C2_3_scores.shape', C1_C2_3_scores.shape)\n",
        "print('C4_7_C5_scores.shape', C4_7_C5_scores.shape)\n",
        "print('pos_vs_neg_scores.shape', pos_vs_neg_scores.shape)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_true.shape (59490,)\n",
            "eight_classes_scores.shape (59490, 8)\n",
            "C1_C2_3_scores.shape (59490, 3)\n",
            "C4_7_C5_scores.shape (59490, 3)\n",
            "pos_vs_neg_scores.shape (59490, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkep8xq7L0ut",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's combine the outputs from all models:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlqQ0p3SuIKT",
        "colab_type": "code",
        "outputId": "71661dd7-3fe5-441e-ef62-4023f45d5fd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X = np.hstack((eight_classes_scores,C1_C2_3_scores,C4_7_C5_scores,pos_vs_neg_scores))\n",
        "print(X.shape)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(59490, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyjy7fS8L_tB",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's add y_true as the last column:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eilt6tSm0cQP",
        "colab_type": "code",
        "outputId": "b7eca7de-0e88-4567-f155-36d21e510949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y = np.reshape(y_true, (len(y_true),-1)) # reshape from 1D to 2D with isngle column\n",
        "train_data = np.hstack((X, y))\n",
        "print(train_data.shape)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(59490, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvNUXiIHt9TV",
        "colab_type": "text"
      },
      "source": [
        "#### **Let's download the train_data into a CSV file on GoogleDrive:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEMCAtvaTYZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dest = 'gdrive/My Drive/Capstone/train_data.csv'\n",
        "np.savetxt(dest, train_data, delimiter=\",\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is5M8gnF4nml",
        "colab_type": "text"
      },
      "source": [
        "# **Next Steps:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qol8Kd1G_J0N",
        "colab_type": "text"
      },
      "source": [
        "#### As a final step, before testing our model, we will use the combined 16 predicted probabilities as features to train a SVM or other simple classification model, to learn to predict either negative or the correct bacterial species, from the probabilities produced by the 4 models above and stored together with y_true in a CSV file on GoogleDrive."
      ]
    }
  ]
}